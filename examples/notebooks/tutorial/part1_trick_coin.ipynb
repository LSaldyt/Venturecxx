{
 "metadata": {
  "name": "",
  "signature": "sha256:613fe23fef96506e7b4435132249310841e387c7993b5c4f8c932af0db0a99c1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Venture Tutorial: Part 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Owain Evans (07.2014)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jump to:\n",
      "- Part 1: Tricky Coin\n",
      "- [Part 2: Gaussians](/notebooks/part2_gaussian.ipynb).\n",
      "- [Part 3: Curve-fitting](/notebooks/part3_regression.ipynb)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Started"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Tips for using the Notebooks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most important: **run cells in sequence**. In each section, later cells depend on previous cells and won't work unless previous cells have been run. \n",
      "\n",
      "If you need to restart the kernel in a section, re-run the cells from the start of the section. Sections usually begin with a cell containing Python `import` statements. You don't need to go back further than this cell.\n",
      "\n",
      "Don't run cells twice if they contain Venture directives. If you do, go back to the start of the section. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Imports for running Venture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We import the Python module `ip_parallel`. This contains the Python Class `MRipl` (along with some utilities) for parallel, interactive use of Venture via IPython Parallel. This also contains the IPython magic command ` %%venture ` for entering Venture programs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_puma_church_prime_ripl, venture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before working with MRipl (which enables parallel inference), we will use Venture via a single RIPL. RIPL stands for 'Read Infer Predict Layer' and is the probablistic programming analog of a [REPL](http://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop).\n",
      "\n",
      "The constructor `make_ripl` creates a fresh RIPL instance. 'Puma' in the cell below indicates the choice of the 'Puma' (C++) vs. 'Lite' (Python) backends. (For this tutorial we will interact with both backends only via the intermediary of Python)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_ripl = make_puma_church_prime_ripl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Is it a trick coin?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Defining a model, constructing a RIPL"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "A magician flips a coin. What is the probability that the coin is <b>tricky</b> (i.e. unfair) given a sequence of flips?\n",
      "\n",
      "We answer this question in three steps: <ol><li>construct a simple hierarchical Bayesian model, <li>condition the model on observed coin-flips, <li> infer the probability that the coin is tricky.</ol> \n",
      "\n",
      "In our model, we draw the boolean variable `is_tricky` from a Bernoulli trial (via `flip`) to represent our prior probability that the coin is tricky. If `is_tricky` is True, we'll generate a random value for the bias `coin_weight`. Otherwise the bias is simply 0.5.\n",
      "\n",
      "-----\n",
      "\n",
      "We first construct a Python object `ripl`, via `make_ripl`. This Python object is our interface with a Venture RIPL. We manipulate this object with Python's object-method syntax (e.g. `ripl.assume( ... )` or via the IPython script magic (e.g. `[assume ...]`. \n",
      "\n",
      "In the following cells we use the IPython magic ` %%venture `. We add model assumptions via the Venture directive `assume` (analogous to Lisp's `define`), which has the form:\n",
      "```python\n",
      "[assume <symbol> <expression>]\n",
      "```\n",
      "The outputs of this second cell are the RIPL's initial values for `is_tricky` and `coin_weight` respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ripl = make_ripl()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ripl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture ripl\n",
      "[clear]\n",
      "[assume is_tricky (flip 0.25)]\n",
      "[assume coin_weight (if is_tricky (uniform_continuous 0 1) .5 ) ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture ripl\n",
      "[sample coin_weight]\n",
      "[sample (list (flip coin_weight) (flip coin_weight) (flip coin_weight))]\n",
      "[sample (and (flip coin_weight) (flip coin_weight))]\n",
      "[sample (+ 5 coin_weight)]\n",
      "[sample (if (> .55 coin_weight) 10 100)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ripl.print_directives()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can print all `assume` directives at any time using the method `<ripl>.print_directives('assume')`. This will be useful later when we have lots of directives."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----------------\n",
      "Having defined our model assumptions we can generate data by forwarding sampling. We use the procedure `sample`, which draws i.i.d from any model expression. We will think of the boolean value 'True' as 'Heads' and 'False' as 'Tails'. (Note that we've used the `flip` stochastic procedure in two ways. First for our prior on the coin being tricky (above) and second for a Bernoulli trial given the coin weight).\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture ripl\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The output for this cell are samples from a Bernoulli trial on `coin_weight`. If the coin is NOT tricky, then True and False are equally likely outcomes and so we expect a mix. \n",
      "\n",
      "The above cells use the IPython magic `%%venture` to interact with the RIPL named `ripl`. We can also use Python's object-method syntax. Expressions are entered as Python strings. \n",
      "\n",
      "(The main directives `assume`, `observe`, and `infer` each mutate the RIPL. They have square brackets in the Lisp syntax and are methods of the Python object. Non-mutating stochastic procedures like `flip` or `uniform_continuous` have normal parentheses and are not Python methods.)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_sample = ripl.sample('(flip coin_weight)')\n",
      "print ripl.sample('(flip coin_weight)')\n",
      "print ripl.sample( '(flip %.2f)' % .99 )\n",
      "ripl.sample('(flip .01)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "my_sample"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above examples generate coin-flips via the expression `(flip coin_weight)`. We can use other expressions to generate coin flips:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use Venture's list and array constructors\n",
      "print ripl.sample('(list (flip coin_weight) (flip coin_weight) )',True)\n",
      "print ripl.sample('(array (flip coin_weight) (flip coin_weight) )'), '\\n'\n",
      "\n",
      "# create a stochastic procedure (here a thunk) and call it\n",
      "ripl.assume('f', '(lambda () (flip coin_weight) )')\n",
      "print ripl.sample('(array (f) (f))')\n",
      "print ripl.sample('(and (f) (f))')\n",
      "print ripl.sample('(if (f) 1 0)'), '\\n'\n",
      "\n",
      "# create a function mapping bools to 'heads'/'tails' strings\n",
      "ripl.assume('bool_to_coin', '(lambda (bool) (if bool (quote heads) (quote tails)))')\n",
      "print ripl.sample('(array (bool_to_coin (f)) (bool_to_coin (f)) (bool_to_coin (f)) )')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Ripl to MRipl"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The state of a Venture RIPL includes a single value for each random variable in the model. But the Bayesian posterior would assign probabilities to every possible value of a variable. This motivates using a _set_ of RIPLs instead of a single one. With a set of RIPLs, each having the same model but different random seeds, we have quick access to an approximate distribution on values. \n",
      "\n",
      "The set of RIPLs is called an MRipl (multi-RIPL). The syntax for interacting with an MRipl is identical to that for a RIPL. As with RIPLs, we have a choice of Venture backend. We also need to specify the number of RIPLs in the MRipl, which we set to '80' for this example."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl\n",
      "v=MRipl(80,local_mode=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v\n",
      "[clear]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v 10\n",
      "[assume is_tricky (flip 0.1)]\n",
      "[assume coin_weight (if is_tricky (uniform_continuous 0 1) 0.5)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The cell above defines the 'Tricky Coin' model above for each of the 80 RIPLs. The RIPLs are independent samples from the same prior distribution: their values for `is_tricky` and `coin_weight` will vary. We control how many RIPL outputs are displayed with the second argument to `%%venture`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we can also use *print_directives* for MRipls\n",
      "v.print_directives()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------\n",
      "\n",
      "In the following cells we generate data from the model. A single `sample` procedure will now generate a coin flip for each of the 80 RIPLs and so it's convenient to avoid displaying all of them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v 10\n",
      "[sample (flip coin_weight)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Using the Python object-method syntax, an MRipl's output is in a Python list. \n",
      "# We slice the list to avoid showing all 80 coin flips.\n",
      "list_samples = v.sample('(flip coin_weight)')\n",
      "len(list_samples)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'is_tricky: ', v.sample('is_tricky')[:8]\n",
      "print ' 1st flip: ', v.sample('(flip coin_weight)')[:8]\n",
      "print ' 2nd flip: ', v.sample('(flip coin_weight)')[:8]\n",
      "print ' 3rd flip: ', v.sample('(flip coin_weight)')[:8]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the MRipl provides 80 independent samples from our prior, we can use it to approximate properties of our prior.\n",
      "\n",
      "First we estimate the expectation of the coin being True (where the actual value is 0.5 due to symmetry).\n",
      "\n",
      "Below we use the `snapshot` method for `MRipl` to plot a histogram and Gaussian kernel density estimator (GKDE) of `coin_weight` values. (The KDE is misleading here because the distribution has such a sharp peak at 0.5). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Proportion True/Total flips: ', np.mean( v.sample('(flip coin_weight)') )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Before inference: P(is_tricky)= ', np.mean(v.sample('is_tricky'))\n",
      "prior_samples = v.snapshot( exp_list=('coin_weight',), plot=True, xlims_ylims=((0,1),) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Conditioning on Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we have simulated from a simple probabilistic model (Tricky Coin). This could easily be done in a standard, non-probabilistic language (e.g. Python or C++). The advantage of Venture comes in conditioning the model on data and doing approximate Bayesian inference.\n",
      "\n",
      "We condition using the directive `observe`, which has the form:\n",
      "```python\n",
      "[observe <expression> <value>]\n",
      "```\n",
      "For our Tricky Coin model, we condition on a series of flips of the coin.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v\n",
      "[observe (flip coin_weight) true]\n",
      "[observe (flip coin_weight) true]\n",
      "[observe (flip coin_weight) true]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Be careful not to run this cell twice. This would add six `observe` directives instead of three. (What happens if we run a set of `assume` directives twice?). To keep track of our observations, we use `print_directives` to print all `observe` directives:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.print_directives('observe')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having observed three 'Heads' outcomes, we expect the probability that the coin is tricky to increase. The `observe` directive does not itself alter this probability (exercise: test this for yourself). To alter the prior probability of `is_tricky` we use the `infer` directive to run approximate Bayesian inference on the conditioned model.\n",
      "\n",
      "The default version of `infer` has an integer argument for the number of Markov chain transitions. Here we run  50 transitions and then plot the new MRipl distribution over `coin_weight`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.infer(50)\n",
      "\n",
      "print 'P(is_tricky) = ', np.mean(v.sample('is_tricky'))\n",
      "\n",
      "posterior_3heads = v.snapshot( ('coin_weight',) )\n",
      "out = v.compare_snapshots( (prior_samples, posterior_3heads) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After three 'Heads' the estimated probability of a tricky coin has increased slightly. From the plots, we see that the distribution on `coin_weight` is less peaked on '0.5' and that there is more probability mass in the right tail (and less in the left). \n",
      "\n",
      "We now add two more observations of 'Heads' outcomes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v\n",
      "[observe (flip coin_weight) true]\n",
      "[observe (flip coin_weight) true]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can review all observations (five 'Heads' outcomes):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.print_directives('observe')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.infer(50)\n",
      "\n",
      "print 'P(is_tricky) = ', np.mean(v.sample('is_tricky'))\n",
      "posterior_5heads = v.snapshot( ('coin_weight',) )\n",
      "out = v.compare_snapshots( (prior_samples, posterior_5heads) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After this inference we can use `sample` again to generate coin flips. This data will reflect our new distribution on the value of `coin_weight`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v 8\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having observed 5 'Heads' in a row, what happens if we now observe 5 'Tails'?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v\n",
      "[observe (flip coin_weight) false]\n",
      "[observe (flip coin_weight) false]\n",
      "[observe (flip coin_weight) false]\n",
      "[observe (flip coin_weight) false]\n",
      "[observe (flip coin_weight) false]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.infer(50)\n",
      "v.print_directives('observe')\n",
      "print 'P(is_tricky) = ', np.mean(v.sample('is_tricky'))\n",
      "\n",
      "posterior_5heads_5tails = v.snapshot( ('coin_weight',) )\n",
      "out = v.compare_snapshots( (posterior_5heads, posterior_5heads_5tails) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_updates = v.compare_snapshots((prior_samples,\n",
      "                                  posterior_3heads,\n",
      "                                  posterior_5heads, \n",
      "                                  posterior_5heads_5tails) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Summary of Tricky Coin Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above example, we used `observe` directives and `infer` directives incrementally. For instance we observed 3 'Heads' and did some inference, and then made more observations and did more inference. We can also add all `observe` directives at once and then do inference on all of them. \n",
      "\n",
      "The next cell recapitulates all the modeling and inference above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl\n",
      "v=MRipl(80,local_mode=True)\n",
      "v.assume('is_tricky', '(flip 0.1)')\n",
      "v.assume('coin_weight', '(if is_tricky (uniform_continuous 0 1) 0.5)')\n",
      "\n",
      "coin_flips = (True,True,True,True,True,False,False,False,False,False)\n",
      "for coin_flip in coin_flips:\n",
      "    v.observe('(flip coin_weight)',coin_flip)\n",
      "    \n",
      "v.infer(200)\n",
      "v.print_directives('observe')\n",
      "print '\\n\\nP(is_tricky) = ', np.mean(v.sample('is_tricky'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tricky Coin Variations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is simple to define variations on the Tricky Coin model above. This time, we raise the prior probability of the coin being tricky. We also change the prior on `coin_weight` given that the coin is tricky."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl\n",
      "v=MRipl(80,local_mode=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v\n",
      "[assume is_tricky (flip 0.5)]\n",
      "[assume coin_weight (if is_tricky (beta .3 .3) 0.5)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v 8\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]\n",
      "[sample (flip coin_weight)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize the impact of these changes to the generative model by comparing snapshots of the prior for the old and new models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_old_prior = MRipl(80,local_mode=True)\n",
      "v_old_prior.assume('is_tricky', '(flip 0.1)')\n",
      "v_old_prior.assume('coin_weight', '(if is_tricky (uniform_continuous 0 1) 0.5)')\n",
      "\n",
      "v_new_prior = MRipl(80,local_mode=True)\n",
      "v_new_prior.assume('is_tricky', '(flip 0.5)')\n",
      "v_new_prior.assume('coin_weight', '(if is_tricky (beta .3 .3) 0.5)')\n",
      "\n",
      "old_prior = v_old_prior.snapshot('coin_weight')\n",
      "new_prior = v_new_prior.snapshot('coin_weight')\n",
      "\n",
      "out = v_new_prior.compare_snapshots((old_prior,new_prior))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now condition the new prior on 3 'Heads'. This has a big impact on the distribution on `coin_weight`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for observation in range(3):\n",
      "    v_new_prior.observe('(flip coin_weight)',True)\n",
      "    \n",
      "v_new_prior.infer(200)\n",
      "\n",
      "v.print_directives('observe')\n",
      "print '\\n P(is_tricky)= ', np.mean(v_new_prior.sample('is_tricky'))\n",
      "print 'P(next flip is heads)= ', np.mean(v_new_prior.sample('(flip coin_weight)'))\n",
      "\n",
      "new_posterior_3heads = v_new_prior.snapshot('coin_weight')\n",
      "out = v_new_prior.compare_snapshots( (new_prior,new_posterior_3heads))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for observation in range(2):\n",
      "    v_new_prior.observe('(flip coin_weight)',True)\n",
      "    \n",
      "v_new_prior.infer(200)\n",
      "\n",
      "v.print_directives('observe')\n",
      "print '\\nP(is_tricky)= ', np.mean(v_new_prior.sample('is_tricky'))\n",
      "print '\\nP(next flip is heads)= ', np.mean(v_new_prior.sample('(flip coin_weight)'))\n",
      "\n",
      "new_posterior_5heads = v_new_prior.snapshot('coin_weight')\n",
      "out = v_new_prior.compare_snapshots( (new_prior,new_posterior_5heads))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# illustrating a quick way to visualize our prior assumptions, namely, (beta .3 .3)\n",
      "v=MRipl(200,local_mode=True)\n",
      "out = v.snapshot('(beta .3 .3)',plot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Switching coins over time"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl\n",
      "v=MRipl(50,local_mode=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous models a single coin was tossed repeatedly. Now imagine that the magician switches between two coins (one fair and one tricky). So there's always a trick coin (with unknown weight `tricky_coin_weight`) and for every flip the magician makes an unobserved choice between the tricky and fair coin.\n",
      "\n",
      "The model is a *Hidden Markov Model*. Successive choices of coin are generated via the recursive procedure `pick_tricky`. This function corresponds to the transition matrix of a discrete HMM. The procedure `flip_coin` corresponds to the observation matrix of an HMM. \n",
      "\n",
      "We use the higher-order function `mem` to memoize coin-choices `(pick_tricky t)` and coin-flips `(flip_coin t)`. This makes the choice of coin at time `t` and the associated coin-flip `(flip_coin t)` fixed for a single run of the program. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture v 0\n",
      "[assume tricky_coin_weight (beta .3 .3)]\n",
      "\n",
      "[assume pick_tricky (mem (lambda (t) \n",
      "                          (if (= t 0) \n",
      "                           (flip)\n",
      "                           (if (pick_tricky (- t 1)) \n",
      "                            (flip .7) \n",
      "                            (flip .3) ) ) ) ) ]\n",
      "\n",
      "[assume flip_coin (mem (lambda (t)\n",
      "                        (if (pick_tricky t) \n",
      "                          (flip tricky_coin_weight) (flip 0.5) ) )) ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We sample a series of coin-flips from the MRipl. We use the `predict` directive rather than using `sample`. Calling `[sample (flip_coin 0)]` would not memoize the value `(pick_tricky 0)` or the outcome of flipping that coin. So our value for `[sample (flip_coin 1)]` would not depend on the value for `(pick_tricky 0)` in the way the HMM specifies. Using `predict` solve this problem: it provides a sample and allows the sample to be memoized."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tricky_coin_weights = v.sample('tricky_coin_weight')\n",
      "\n",
      "all_flips = np.array( [v.predict('(flip_coin %i)' % t) for t in range(15)] ).T\n",
      "\n",
      "def bool_str(boolean): return 'T' if boolean else 'F'\n",
      "\n",
      "for count,(weight,flips) in enumerate( zip(tricky_coin_weights,all_flips)[:9] ):\n",
      "    print 'Ripl # %i' % count\n",
      "    print 'Tricky Coin-weight: %.2f,\\nflips: %s \\n' % (weight, ' '.join( map(bool_str, flips) ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We condition this model on 5 'Heads'. We now need to specify the time index `t` of the flip. The loop in the cell below unrolls to `v.observe('(flip_coin 0)',True)` where `t` goes up to 3. \n",
      "\n",
      "We do inference with the directive `v.infer(<number_transitions>)`. This calls the same default inference program we used above, which is single-site Metropolis-Hastings on all random choices in the program. Venture supports inference programs that may be more suitable to this HMM, e.g. Particle Filters, MH with Particle Gibbs proposals, and incremental MH (where we MH on a sliding window of hidden states). There is some use of inference programming in Part 4 of the this tutorial. (See also the [Venture Paper](http://arxiv.org/abs/1404.0099)).\n",
      "\n",
      "After running inference, we compute the probability that the next flip `(flip_coin 5)` is Heads. This probability is lower than the most recent Tricky Coin model, which had the same `(beta .3 .3)` prior on the tricky coin's weight. This is because the magician can always switch to the fair coin.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prior = v.snapshot('tricky_coin_weight')\n",
      "\n",
      "for t in range(4):\n",
      "    v.observe('(flip_coin %i)'%t,True)\n",
      "    \n",
      "v.infer(100)\n",
      "\n",
      "v.print_directives('observe')\n",
      "print '\\nP( \"next flip is Heads\" ) = ', np.mean(v.sample('(flip_coin 5)'))\n",
      "print '\\nP( \"Coin choice at t=0 is Tricky Coin\" ) = ', np.mean(v.sample('(pick_tricky 0)')), '\\n'\n",
      "\n",
      "posterior=v.snapshot('tricky_coin_weight')\n",
      "out = v.compare_snapshots( (prior,posterior) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.infer(100)\n",
      "posterior2 = v.snapshot('tricky_coin_weight')\n",
      "out = v.compare_snapshots( (prior,posterior2) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.infer(100)\n",
      "posterior3 = v.snapshot('tricky_coin_weight')\n",
      "out = v.compare_snapshots( (prior,posterior3) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}