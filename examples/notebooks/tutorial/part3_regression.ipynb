{
 "metadata": {
  "name": "",
  "signature": "sha256:644d6d7c752db2b28544f6b3c66656eb54d35a7b1c2153b30e5189c1b5124d49"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Venture Tutorial Part 3"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Owain Evans (06.2013)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Jump to:\n",
      "- [Part 1: Tricky Coin](/notebooks/part1_trick_coin.ipynb)\n",
      "- [Part 2: Gaussians](/notebooks/part2_gaussian.ipynb).\n",
      "- Part 3: Curve-fitting\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Curve-fitting with MRipl and map_proc"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Quadratic Model, Plotting the Prior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we implement 2D curve-fitting using Bayesian inference on a generative model for curves. We begin with a generative model for quadratic functions called `simple_quadratic`.\n",
      "\n",
      "We model the x-values (the input variable for the curve `f`) as drawn from a fixed Gaussian:\n",
      "```python\n",
      "[assume x_d (lambda () (normal 0 5))]\n",
      "```\n",
      "We place independent Gaussian priors on the quadratic coefficients (`w0`,`w1`,`w2`) and a Gamma prior on the noise parameter `noise`. We then model the output y(x) as N( f(x), noise):\n",
      "```python\n",
      "[assume y_x (lambda (x) (normal (f x) noise) ) ]\n",
      "```\n",
      "Our initial aim is to learn the quadratic coefficients and not to learn the joint distribution on the x- and y-values. (We explicitly model the x-values to make sampling from the model easy. Here we fix the distribution to be `(normal 0 5)` but we can easily move to a model which learns this distribution).  \n",
      "\n",
      "(The variable `model_name` has no inferential role in the model and is used to provide titles for plots)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl,mk_p_ripl,venture\n",
      "from venture.venturemagics.reg_demo_utils import *\n",
      "ripl=mk_p_ripl()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%venture ripl\n",
      "[assume x_d (lambda () (normal 0 5))]\n",
      "\n",
      "[assume w0 (normal 0 3) ]\n",
      "[assume w1 (normal 0 1) ]\n",
      "[assume w2 (normal 0 .3) ]\n",
      "[assume f (lambda (x) (+ w0 (* w1 x) (* w2 (* x x)) ) ) ]\n",
      "\n",
      "[assume noise (gamma 2 1) ]\n",
      "[assume y_x (lambda (x) (normal (f x) noise) ) ]\n",
      "\n",
      "[assume model_name (quote simple_quadratic)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "visualize_prior=MRipl(80,local_mode=True)\n",
      "out = visualize_prior.snapshot('(gamma 2 1)',plot=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ripl.print_directives()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having defined our generative model we want to visualize our prior on curves. For the [Tricky Coin](/part1_trick_coin.ipynb) and [Gaussian](/part2_gaussian.ipynb) parts of this tutorial we used simple plots that are built into the `snapshot` method. For curve-fitting (and for almost any other use of Venture) we need to make custom plots. \n",
      "\n",
      "We use a plotting function `plot_conditional` (defined in the module `reg_demo_utils`) to display the RIPL's current value for the curve `f` and for the conditional distribution. The conditional distribution is `(y_x <x_value>)` in the Venture program and  \n",
      "$$P(y(X) | x= X )$$\n",
      "in mathematical notation. This is the distribution on possible values for $y(X)$ for each $X$ in a grid of x-values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out=plot_conditional(ripl, x_range=(-8,8), number_xs=20, number_reps=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The left plot shows the curve `f` with 1SD noise intervals (from `noise`). The middle plot samples from `(y_x <x_value>)` over a grid of x-values and displays each y-value along with means and 1SD curves. The right plot is a heatmap of the samples from the middle plot.\n",
      "\n",
      "This kind of plotting function does not depend on the functional form of `f`. Values are generated by repeatedly sampling `(f <x_value>)` and `(y_x <x_value>)` on a grid of x-values. The function `my_plot_conditional` below is a simpler version of `plot_conditional`. \n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def my_plot_conditional(ripl,x_range=(-8,8),number_xs=30):\n",
      "    xr = np.linspace(x_range[0],x_range[1],number_xs)\n",
      "    f_xr = [ripl.sample('(f %f)' %x) for x in xr ]\n",
      "    y_xr = [ripl.sample('(y_x %f)' %x) for x in xr ]\n",
      "    \n",
      "    fig,ax = plt.subplots(1,2,figsize=(4,2.5))\n",
      "    ax[0].plot(xr,f_xr) ## ax[0].plot(xr,f_l,xr,f_u)\n",
      "    ax[1].scatter(xr,y_xr) ##ax[1].plot(xr,y_mean,xr,y_l,xr,y_u)\n",
      "    return xr,f_xr\n",
      "\n",
      "out = my_plot_conditional(ripl,(-10,8),20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could also display `f` and `y_x` using the quadratic functional form and sampling the relevant parameters from the model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'f(x) = %.2f + %.2fx + %.2fx^2' % (ripl.sample('w0'),ripl.sample('w1'),ripl.sample('w2'))\n",
      "print 'y = N( f(x), %.2f)' % ripl.sample('noise')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Conditioning the model on data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We generate some $(x,y)$ pairs from a curve $f$ and then condition the model on them. Though we could generate data from the Venture program itself, we use Python for convenience.\n",
      "\n",
      "We generate x-values as a mixture of two Gaussians. The curve is $f(x)=-2x$, and $y \\sim N(f(x),0.6)$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate x values\n",
      "xs=list(np.random.normal(3,.7,3)) + list(np.random.normal(-1,1.4,8))\n",
      "\n",
      "# specify f and y_x\n",
      "f = lambda x:-2*x\n",
      "y_x = lambda x: np.random.normal(f(x),.6)\n",
      "\n",
      "# compute y values from x and f(x)     \n",
      "ys = [y_x(x) for x in xs]    \n",
      "\n",
      "# store values as variable 'data'\n",
      "data = zip(xs,ys)\n",
      "\n",
      "# split the data set in two\n",
      "data0,data1 = data[:6],data[6:]\n",
      "\n",
      "# plot (x,y) pairs\n",
      "fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
      "[ax.scatter( *zip(*data_i) ) for data_i in (data0,data1)];\n",
      "ax.set_title('Freshly generated data')\n",
      "\n",
      "# pre-generated data\n",
      "data0 = ([ 4.13, -8.34],\n",
      "         [ 3.01, -6.05],\n",
      "         [ 3.33, -5.9 ],)\n",
      "       \n",
      "# plot pre-generated data\n",
      "fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
      "ax.scatter( *zip(*data0) , color='m')\n",
      "ax.set_title('Pre-generated data0');\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We condition the model on the $(x,y)$ pairs from `data0`. We then run our default MH inference program. We compare the prior and posterior conditionals on a grid of points given by `x_range`. Since the data is generated by a quadratic $f(x) = -2x$ with parameters that are plausible given our prior, we expect inference to find a curve that fits the points well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot prior conditional (before conditioning)\n",
      "prior=plot_conditional(ripl,data=data0,x_range=(-8,8),number_xs=20,number_reps=15)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# condition model on x,y pairs\n",
      "for (x,y) in data0:\n",
      "    ripl.observe('(y_x %f)' % x , '%f' % y )\n",
      "    \n",
      "ripl.print_directives('observe')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot posterior conditional (after inference)\n",
      "ripl.infer(50)\n",
      "out=plot_conditional(ripl,data=data0, x_range=(-8,8), number_xs=20, number_reps=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "3. Curve-fitting with MRipl"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plot above shows that we inferred a curve `f` that fits the points well. However, after observing only 6 points that fall mostly within the interval $(-5,5)$ it is probably not possible to pin down the higher-order terms of the quadratic. (Our uncertainty would be much greater if our model allowed a larger class of curves). In order to measure uncertainty in the posterior over `f` and the conditional posterior, we work with an MRipl.\n",
      "\n",
      "For convenience, we load the MRipl with a string, `simple_quadratic_model` (stored in `reg_demo_utils`) and we use a function `observe_xy` that loops over the $(x,y)$ points in `data0` and adds `observe` directives to the MRipl.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl,mk_p_ripl,venture\n",
      "from venture.venturemagics.reg_demo_utils import *\n",
      "\n",
      "data0 = ([ 4.13, -8.34],\n",
      "         [ 3.01, -6.05],\n",
      "         [ 3.33, -5.9 ],\n",
      "         [ 2.5,  -5.05],\n",
      "         [ 2.01,  -4.2],\n",
      "         [ 1.8,   -3.72],)\n",
      "\n",
      "v=MRipl(20, local_mode=True)\n",
      "out = v.execute_program(simple_quadratic_model);\n",
      "print simple_quadratic_model\n",
      "\n",
      "out = observe_xy(v,data0);\n",
      "print 'data0: ', data0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Though we have 'observed' the points `data0`, we have not yet run `infer` and so the MRipl is still in its prior state. We visualize this state by mapping our single-ripl plotting functions across all ripls in the MRipl. We use the MRipl method `map_proc` which has the form:\n",
      "```python\n",
      "<mripl>.map_proc( no_ripls, proc, *proc_args, **proc_kwargs)\n",
      "```\n",
      "We select how many ripls to map over with `no_ripls` or map across all ripls by setting the first positional argument to `'all'`. The argument `proc` is a procedure with a ripl as its first argument. If the procedure has other arguments we enter them in `proc_args` and `proc_kwargs`. (Side-note: If using MRipl with IPython Parallel, the procedure needs to be serializable. We use the `interactive` decorator to change the `__module__` of `proc` to `__main__`.)  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sample_params(r):\n",
      "    return (r.sample('w0'),r.sample('w1'),r.sample('w2')), r.sample('noise') \n",
      "\n",
      "outs = v.map_proc(3,sample_params)\n",
      "for w,noise in outs:\n",
      "    print 'f(x) = %.2f + %.2fx + %.2fx^2' % w, '    y = N(f(x),%.2f)' % noise\n",
      "    print '-----'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# mapping a plotting function across ripls\n",
      "out = v.map_proc(2,my_plot_conditional)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# visualize sample curves from our prior via plot_conditional\n",
      "out=v.map_proc(4,plot_conditional,data=data0,x_range=(-8,8),number_xs=20,number_reps=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As noted above, it's not straightforward with a single ripl to visualize our Bayesian uncertainty over different curves `f` and conditional distributions `y_x`. With an MRipl, we use a plotting function `predictive` that samples `(y_x <x_value>)` on a grid of x-values for each ripl in the MRipl. \n",
      "\n",
      "The plots below show our prior uncertainty. We then run inference and visualize the posterior conditional and posterior predictive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out=predictive(v,data=data0,x_range=(-8,8),number_xs=30,number_reps=15)\n",
      "\n",
      "# note that because the prior on the quadratic parameters are Normal distributions \n",
      "# symmetric about 0, our predictive distribution is symmetric"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run inference on the MRipl on *data0*\n",
      "v.infer(500)\n",
      "out=v.map_proc(6, plot_conditional,data=data0,number_xs=10,number_reps=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out = predictive(v,data=data0, number_xs=20,number_reps=30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Fourier Model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "1. Prior and Posterior on Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can easily adjust the functional form of the curves in the generative model. We move from quadratics to the form:\n",
      "$$f(x) = w_0 + w_1 \\sin(\\omega x + \\theta)$$\n",
      "The conditional distribution is still:\n",
      "$$y(x) = N( f(x), noise)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl,display_directives,mk_p_ripl,venture\n",
      "from venture.venturemagics.reg_demo_utils import *\n",
      "data0 = ([ 4.13, -8.34],\n",
      "         [ 3.01, -6.05],\n",
      "         [ 3.33, -5.9 ],\n",
      "         [-3.07,  6.22],\n",
      "         [-0.53,  0.92],\n",
      "         [-1.57,  2.96],)\n",
      "\n",
      "print simple_fourier_model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We condition on the same dataset as above and run inference on the Fourier model. Recall from above that `data0` was generated using $y=-2x$. We can compare the logscores of the two models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vf = MRipl(10)\n",
      "vf.execute_program(simple_fourier_model)\n",
      "observe_xy(vf,data=data0)\n",
      "vf.infer(1000)\n",
      "out = vf.map_proc(4, plot_conditional, data=data0, number_xs=20, number_reps=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vs = [v,vf]\n",
      "[display_logscores(r) for r in vs];\n",
      "# display_logscores function in *reg_demo_utils* prints the mean and max logscore across ripls\n",
      "# it uses the *ripl* method *get_global_logscore*"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "____"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "2. Model Selection and the Quadratic-Fourier Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The examples above illustrate model comparison using multiple MRipls. We can condition models on the same data and make qualitative comparisons via plots and quantative comparisons via the `get_global_logscore` method or from statistics specialized for a given model (e.g MSE for curve-fitting).\n",
      "\n",
      "Venture also makes it easy to combine models. For curve-fitting we simply add a prior on the functional form. This enables us to do Bayesian model selection over the `simple_quadratic` and `simple_fourier` models above.\n",
      "\n",
      "We also expand on the models above by adding a hyper-prior over the prior on the x-values. By learning the marginal $P(x)$ as well as the conditional $P(y|x)$ we can answer a wider range of queries about the joint $P(x,y)$. In the `quad_fourier` model below, we model the x-values as follows:\n",
      "$$ x \\sim student(\\nu)$$\n",
      "\n",
      "$$ \\nu \\sim gamma(10,1)$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl,display_directives,mk_p_ripl,venture\n",
      "from venture.venturemagics.reg_demo_utils import *\n",
      "data0 = ([ 4.13, -8.34],\n",
      "         [ 3.01, -6.05],\n",
      "         [ 3.33, -5.9 ],\n",
      "         [-3.07,  6.22],\n",
      "         [-0.53,  0.92],\n",
      "         [-1.57,  2.96],)\n",
      "\n",
      "quad_fourier =  x_model_t + quad_fourier_model\n",
      "print quad_fourier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = MRipl(10, local_mode = True)\n",
      "v.execute_program( quad_fourier )\n",
      "observe_xy(v, data=data0)\n",
      "v.infer(1000)\n",
      "out=v.map_proc(4, plot_conditional,data=data0,x_range=(-5,5),number_xs=20, number_reps=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out=predictive(v,data=data0,x_range=(-6,6),number_xs=15,number_reps=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model selection\n",
      "fourier_prob = np.mean( v.sample('model') )\n",
      "print 'Inferred model probability: quadratic %.2f, fourier %.2f \\n' % (1-fourier_prob,fourier_prob)\n",
      "\n",
      "\n",
      "# parameter inference\n",
      "no_sample_ripls = 3\n",
      "print '\\nSampled parameters for %i ripls in MRipl:'% no_sample_ripls\n",
      "\n",
      "def sample_params(r): return tuple( map(r.sample,('w0','w1','w2','noise')) )\n",
      "\n",
      "for params in v.map_proc(no_sample_ripls,sample_params):\n",
      "    print 'f(x) = %.2f + %.2fx + %.2fx^2     y = N(f(x),%.2f) \\n---\\n' % params \n",
      "    \n",
      "print 'Mean parameter values:\\n'\n",
      "mean_params = tuple( np.mean(sample_params(v),axis=1) )\n",
      "print 'f(x) = %.2f + %.2fx + %.2fx^2     y = N(f(x),%.2f)' % mean_params "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "----"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Quad_Fourier Model on Fourier Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now generate data from $\\sin(0.5x)$ and see if the combined `quad_fourier` model can learn functional form."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat imports\n",
      "from venture.venturemagics.ip_parallel import MRipl,display_directives,mk_p_ripl,venture\n",
      "from venture.venturemagics.reg_demo_utils import *\n",
      "quad_fourier =  x_model_t + quad_fourier_model\n",
      "\n",
      "# generate x values\n",
      "xs=list(np.random.normal(4,1,5)) + list(np.random.normal(-1.5, 1.5,5))\n",
      "\n",
      "# specify f and y_x\n",
      "f = lambda x:np.sin(.5*x)\n",
      "y_x = lambda x: np.random.normal(f(x),.05)\n",
      "\n",
      "# compute y values from x and f(x)     \n",
      "ys = [y_x(x) for x in xs]    \n",
      "\n",
      "# store values as variable 'data'\n",
      "data = zip(xs,ys)\n",
      "\n",
      "# plot (x,y) pairs\n",
      "fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
      "ax.scatter( *zip(*data) )\n",
      "ax.set_title('Sine data');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = MRipl(12, local_mode = True)\n",
      "v.execute_program( quad_fourier )\n",
      "observe_xy(v,data=data)\n",
      "v.infer(1500)\n",
      "out=v.map_proc(4, plot_conditional,data=data,x_range=(-5,5),number_xs=20, number_reps=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out=predictive(v,data=data,x_range=(-6,6),number_xs=15,number_reps=30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model selection\n",
      "fourier_prob = np.mean( v.sample('model') )\n",
      "print 'Inferred model probability: quadratic %.2f, fourier %.2f \\n' % (1-fourier_prob,fourier_prob)\n",
      "\n",
      "\n",
      "# parameter inference\n",
      "no_sample_ripls = 4\n",
      "print '\\nSampled parameters for %i ripls in MRipl:'%(no_sample_ripls)\n",
      "\n",
      "def sample_params(r): return tuple( map(r.sample,('w0','w1','omega','theta','noise')) )\n",
      "f_string ='f(x) = %.2f + %.2f*sin(%.2fx + %.2f)      y = N(f(x),%.2f)'\n",
      "\n",
      "for params in v.map_proc(no_sample_ripls,sample_params):\n",
      "    print f_string % params, '\\n---\\n' \n",
      "    \n",
      "print 'Mean parameter values:\\n'\n",
      "mean_params = tuple( np.mean(sample_params(v),axis=1) )\n",
      "print f_string % mean_params t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}