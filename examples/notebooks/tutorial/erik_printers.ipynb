{
 "metadata": {
  "name": "",
  "signature": "sha256:609004e43330880651846f5be70786fa5d49fd61c3331ff522dc4bcab65df2c8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plan for modeling each nozzle independently. still has time-varying error rate. first version treats errors as iid. later version does a simple model of dependence of nozzles on neighbors: noddles are put into 10-100 groups and each group has a latent state that can be Good or Bad. In the Bad state, clogged rate is higher. (Move on to having kernels of influence, or DP to segment the nozzles.)\n",
      "\n",
      "```python\n",
      "\n",
      "# there are up to 10**4 nozzles, you get data from each one picked out by id (place on 1d line)\n",
      "# can add colors to model later (3 colors)\n",
      "[assume nozzle_state (mem (nozzle_id t) \n",
      "                              (let ( ( previous_state (nozzle_state nozzle_id (- t 1)) ) \n",
      "                                     ( nozzle_group_state (nozzle_group nozzle_id t) )\n",
      "                                  (if \n",
      "                                    (dead previous_state) 'dead\n",
      "                                      (if (clogged previous_state (clogged_transition)\n",
      "                                          (clean_transition nozzle_group_state) ) ) ) ) )) ]\n",
      "# clean transition is then a bernoulli with varying-over-time theta param\n",
      "# if clogged you usually get cleaned before another observation\n",
      "\n",
      "# nozzle_group_state is hidden variable for each of say 10-100 groups. this is \n",
      "# a simple model of dependence of states where (e.g.) bernoulli theta chance \n",
      "# of death or clogging is higher if the nozzle_group state is bad\n",
      "\n",
      "\n",
      "# could separate cleaning events from observations\n",
      "\n",
      "\n",
      "# simple calculation of size of model (so what inference steps must be constant time)\n",
      "N = 10**4 # nozzle-count\n",
      "nozzle_groups = (10,100)\n",
      "observations_per_nozzle = 1000\n",
      "num_hidden_states = 10**4  # i.e. states observable in principle but only 1/1000 are observed\n",
      "                           # we might need to model them because data is irregularly spaced out (like SLAM?)\n",
      "                           # 10**4 would correspond to a state of nozzle after every 10 pages of printing\n",
      "                           \n",
      "\n",
      "num_nozzle_time_states = num_hidden_states * N\n",
      "num_groups_time_states = nozzle_groups * num_hidden_states\n",
      "\n",
      "# use uncollapsed beta-bernoulli for constant time updates - otherwise you have 10**7 or 10**8 flips to count up every time you propose to the params on the rate of failure.                              \n",
      "```                              \n",
      "                                 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Notes on Basic Model (that doesn't model individual nozzles)*\n",
      "Aim is to compare the performance of different printers by analyzing a series of counts of failed nozzles. in the simplest case, the counts come after a fixed number of printed pages (e.g. after every 100 pages). counts could also come at uneven intervals (i.e. varying numbers of pages before next observation of failure count). in the simplest model, nozzles are wiped between observations, where wiping corrects failures. (clogged nozzles are declogged). thus every observation of the count of failures is independent of the others (no markov dependence) apart from the change over time of the failure rate. a more complex model would allow multiple observations before a wiping event, which induce a dependency between failure counts (even when conditioning on the changing failure rate). in the model below, i use regularly spaced intervals and assume that wiping events occur between all observations.\n",
      "\n",
      "failures are poisson in the failure rate. the failure rate is assumed to grow linearly until an unknown switch_point and then plateau. the model below makes the weaker assumption that after the switch_point the rate is a random constant value (rather than plateauing).\n",
      "\n",
      "scaling: add more observations means more calls of (poisson rate). so we have at least linear scaling for a single sample. otoh, i'm not sure how the number of samples needed will scale. \n",
      "\n",
      "below i tried a more complex model, where the rate is either the regular rate above or a 'random rate'. the random rate is assumed to be an unknown constant below (and so what's random is just whether that rate is selected at any timestep) but we could also have a random rate that varies randomly over time. the random rate could be much higher than the regular rate. before we had K observed poisson draws and the only random variables were the rate parameters. now we also have K latent variables for whether the random or regular rate was selected at a timestep. this greatly slows down default inference. if K is large, then the rate parameters (which we actually want to learn) will rarely be proposed to. if we have one very high count (e.g. 100) and the latent is not set to the random rate, then we will infer much too high rate for the regular rate. one way to deal with this is to not model the random rate at all and just switch from a poisson to a heavy-tailed distribution on counts. (not sure which one? pitman-yor is discrete and power-law. could just move to continuous variables and use student-t. my sense is that it's not trivial to get the right kind of heavy-tailed distribution but i haven't tried it). while inference was much slower, it was still possible to get decent results on datasets not to far from actual size.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import *\n",
      "from venture.unit import Analytics\n",
      "\n",
      "def test_random_model():\n",
      "    v=mk_p_ripl()\n",
      "    num_draws = 150\n",
      "    args = dict(\n",
      "        dist_random_event = '(flip .9999)',\n",
      "        prior_random_rate = 100,)\n",
      "    v.execute_program( make_random_model(args) )\n",
      "    \n",
      "    assert int( v.sample('random_rate') ) == int( args['prior_random_rate'] )\n",
      "    g2bs =[v.predict('(g2b {})'.format(i)) for i in range(num_draws)]\n",
      "    rates = map(int, [v.predict('(rate {})'.format(i)) for i in range(num_draws)] )\n",
      "    for r in rates:\n",
      "        assert r == args['prior_random_rate']\n",
      "    assert np.abs( (np.mean(g2bs) - args['prior_random_rate']) ) < 5\n",
      "    assert np.abs( (np.var(g2bs) - args['prior_random_rate']) ) < 30\n",
      "    \n",
      "    \n",
      "    args = dict(\n",
      "                 dist_random_event = '(flip .0001)',\n",
      "                 prior_lam_linear = 1)\n",
      "    v.clear()\n",
      "    v.execute_program( make_random_model(args) )\n",
      "    assert int( v.sample('lam_start')) == args['prior_lam_linear']\n",
      "    assert int( v.sample('lam_coefficient')) == args['prior_lam_linear']\n",
      "    rates = map(int, [v.predict('(rate {})'.format(i)) for i in range(num_draws)] )\n",
      "    print rates[:20]\n",
      "\n",
      "\n",
      "def make_random_model(my_args=None):\n",
      "    \n",
      "    args = dict(\n",
      "        prior_switch_point = '(gamma 1 .01)',\n",
      "        prior_lam_linear = '(gamma 1 .3)',\n",
      "        prior_post_switch = '(gamma 1 .01)',\n",
      "        prior_random_rate = '(gamma 1 .01)',\n",
      "        dist_random_event = '(flip .01)' )\n",
      "    \n",
      "    if my_args: args.update(my_args) \n",
      "    \n",
      "\n",
      "    random_model='''\n",
      "    [assume lam_start (tag (quote hypers) 0 \n",
      "                        {prior_lam_linear} )]\n",
      "    [assume lam_coefficient (tag (quote hypers) 1 \n",
      "                        {prior_lam_linear} )]\n",
      "\n",
      "    [assume switch_point (tag (quote switch) 0 \n",
      "                           {prior_switch_point})]\n",
      "    [assume lam_post_switch (tag (quote hypers) 2 \n",
      "                           {prior_post_switch})]\n",
      "    [assume random_rate (tag (quote random_rate) 0 \n",
      "                           {prior_random_rate})]\n",
      "\n",
      "    [assume random_event (lambda () {dist_random_event})]\n",
      "\n",
      "    [assume rate (mem (lambda (t)\n",
      "                        (tag (quote rate_choice) t\n",
      "                          (if (random_event)\n",
      "                            random_rate\n",
      "                            (regular_rate t)))))]\n",
      "\n",
      "    [assume regular_rate (lambda (t) \n",
      "                          (if (< t switch_point) \n",
      "                            (+ lam_start (* t lam_coefficient))\n",
      "                            lam_post_switch ))]\n",
      "\n",
      "    [assume g2b (lambda (t) (poisson (rate t))) ]\n",
      "    '''.format( prior_switch_point = args['prior_switch_point'], prior_lam_linear = args['prior_lam_linear'],\n",
      "                prior_post_switch = args['prior_post_switch'], prior_random_rate = args['prior_random_rate'],\n",
      "                dist_random_event = args['dist_random_event'])\n",
      "    return random_model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import *\n",
      "from venture.unit import Analytics\n",
      "\n",
      "# basic model without random event\n",
      "switch_prior = '(gamma 1 .01)'\n",
      "lam_hypers = '(gamma 1 .3)'\n",
      "\n",
      "m1='''\n",
      "[assume lam_start (tag (quote hypers) 0 %s )]\n",
      "[assume lam_coefficient (tag (quote hypers) 1 %s )]\n",
      "\n",
      "[assume switch_point (tag (quote switch) 0 %s)]\n",
      "[assume lam_post_switch (tag (quote hypers) 2 (gamma 1 .01))]\n",
      "\n",
      "[assume lam (lambda (t) (if (< t switch_point) \n",
      "                          (+ lam_start (* t lam_coefficient))\n",
      "                          lam_post_switch ))]\n",
      "[assume g2b (lambda (t) (poisson (lam t))) ]\n",
      "'''% (lam_hypers,lam_hypers,switch_prior)\n",
      "\n",
      "# basic model\n",
      "m='''\n",
      "[assume lam_start (tag (quote hypers) 0 (gamma 3 1) )]\n",
      "[assume lam_coefficient (tag (quote hypers) 1 (gamma 3 1) )]\n",
      "[assume lam (lambda (t) (+ lam_start (* t lam_coefficient)))]\n",
      "[assume g2b (lambda (t) (poisson (lam t))) ]\n",
      "\n",
      "'''\n",
      "def generate_synthetic(num_timesteps=200, interval_size=3,\n",
      "                       lam_coefficient = .05, random_rate = 10):\n",
      "    lam_start = .1\n",
      "    switch_point = int(.5*num_timesteps)\n",
      "    lam_post_switch = lam_start + (switch_point *lam_coefficient)\n",
      "    random_event = .05\n",
      "\n",
      "    observes = []; queries = []\n",
      "    poi = lambda lam: np.random.poisson(lam)\n",
      "\n",
      "    for i in range(0,num_timesteps,interval_size):\n",
      "        if i < switch_point:\n",
      "            observes.append( ['(g2b %i)'%i,  poi(lam_start + (i*lam_coefficient))] )\n",
      "        else:\n",
      "            observes.append( ['(g2b %i)'%i, poi(lam_post_switch) ] )\n",
      "        queries.append( '(rate %i)'%i)\n",
      "        \n",
      "        if np.random.binomial(1,random_event):\n",
      "            observes[-1][1] = poi(random_rate) \n",
      "        \n",
      "    print '''Synthetic params: switch_p:{switch_p}, interval_size: {int_size}, lam_start:{lam_start},\n",
      "             lam_coef:{lam_coeff}, lam_post_switch:{lam_post_switch}, random_rate:{random_rate}\n",
      "             '''.format(switch_p=switch_point,\n",
      "                        int_size=interval_size,lam_coeff=lam_coefficient, lam_start=lam_start,\n",
      "                        lam_post_switch=lam_post_switch, random_rate=random_rate)  \n",
      "    return observes,queries\n",
      "\n",
      "ro = lambda x: np.round(x,2)\n",
      "var_names = ('lam_start','lam_coefficient','switch_point','lam_post_switch','random_rate','logscore')\n",
      "\n",
      "def plot_hypers(h,rate=None):\n",
      "    map( h.quickPlot, var_names)\n",
      "    if rate:\n",
      "        for i in range(50):\n",
      "            if '(rate %i)'%i in queries: h.quickPlot('(rate %i)'%i)\n",
      "        \n",
      "        \n",
      "def test_inference_synthetic(sweeps=1, cond_prior=None,runs=3):\n",
      "    my_args = dict(\n",
      "        prior_switch_point = '(gamma 1 .01)',\n",
      "        prior_lam_linear = '(gamma 1 .5)',\n",
      "        prior_post_switch = '(gamma 1 .01)',\n",
      "        prior_random_rate = '(gamma 1 .01)',\n",
      "        dist_random_event = '(flip .01)' )\n",
      "    \n",
      "    model = make_random_model(my_args)\n",
      "    \n",
      "    synth_args= dict(   num_timesteps=num_timesteps, \n",
      "                        interval_size=3,\n",
      "                       lam_coefficient = .05,\n",
      "                       random_rate = 100)\n",
      "    observes,queries = generate_synthetic(**synth_args)\n",
      "    \n",
      "    v=MRipl(2,local_mode=True)\n",
      "    v.execute_program( model )\n",
      "    [v.observe(*obs) for obs in observes]\n",
      "    ana = Analytics(v,queryExps=queries)\n",
      "    \n",
      "    inf_kwargs = dict( runs=runs, infer=infer_prog,simpleInfer=True )\n",
      "    \n",
      "    if cond_prior:\n",
      "        h,mr = ana.runConditionedFromPrior(sweeps,**inf_kwargs)\n",
      "    else:\n",
      "        h,mr = ana.runFromConditional(sweeps,**inf_kwargs)\n",
      "        \n",
      "    snaps = h.historyToSnapshots()\n",
      "    funcs= np.mean, lambda x:x\n",
      "    print '\\n funcs: mean last snapshot, last snapshot\\n'\n",
      "    for var_name in var_names:\n",
      "        print var_name\n",
      "        print [ro(f( snaps[var_name][-1] )) for f in funcs]\n",
      "        \n",
      "    print '\\nabs coeff:', np.abs(np.mean( snaps['lam_coefficient'][-1]) - synth_args['lam_coefficient'])\n",
      "    return h, observes\n",
      "\n",
      "\n",
      "#test_random_model()\n",
      "\n",
      "num_timesteps = 100  # global\n",
      "runs = 3\n",
      "cond_prior = False\n",
      "\n",
      "\n",
      "infer_progs = dict(pgibbs= '(cycle ( (pgibbs hypers one 50 1) (pgibbs switch one 50 1) ) 1)',\n",
      "                   mh_default = '(mh default one 350)', \n",
      "                   mh_cycle_all = '''(cycle ( (mh hypers one 35) (mh switch one 5) \n",
      "                                     (mh rate_choice one 1) (mh random_rate one 1)) 1)''',\n",
      "                   mh_cycle_rate_first = '''(cycle ( \n",
      "                                               (mh rate_choice one {rate_trans}) \n",
      "                                               (mh default one 1000 )) 1)'''.format(rate_trans=num_timesteps*2),\n",
      "                   gibbs_cycle_rate_first = '''(cycle ( \n",
      "                                               (gibbs rate_choice one {rate_trans}) \n",
      "                                               (mh default one 1000 )) 1)'''.format(rate_trans=num_timesteps*2) )\n",
      "\n",
      "my_infer_progs = ['mh_cycle_rate_first','gibbs_cycle_rate_first']\n",
      "\n",
      "hs = []\n",
      "st = time.time()\n",
      "for i in my_infer_progs: \n",
      "    infer_prog = infer_progs[i]\n",
      "    print 'inf prog',infer_prog\n",
      "    %time h,observes = ( test_inference_synthetic( sweeps = 3, cond_prior=cond_prior, runs=runs) )\n",
      "    hs.append( h )\n",
      "    \n",
      "    for obs in observes:\n",
      "        if obs[1]>20: print obs\n",
      "print 'el: %.2f'%(time.time()-st)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = hs[0]\n",
      "h.quickPlot('logscore')\n",
      "h.quickPlot('switch_point')\n",
      "h.quickPlot('lam_coefficient')\n",
      "h.quickPlot('lam_start')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%time h,mr = ana.runConditionedFromPrior(10, runs=2, useMRipl=True, **inf_kwargs)\n",
      "plot_hypers(h)\n",
      "#h.quickPlot('(rate 0)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h.compareSnapshots(names=('lam_coefficient',))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}