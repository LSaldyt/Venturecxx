{
 "metadata": {
  "name": "",
  "signature": "sha256:251f30306a55353657bee640816a573dc0f7b394791949f53df480e39e051ecc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Comparison of sampling methods on a highly-correlated bivariate Gaussian"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below is a comparison of some of the inference methods available to Venture on a correlated bivariate Gaussian - a test case where simple proposals like MH often have trouble. I've chosen the number of iterations for each method so that they have about the same wall clock time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Building the model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "First approach - switch x and mu"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.shortcuts import make_lite_church_prime_ripl\n",
      "from time import time\n",
      "from utils import bivariate_normal_contour"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The goal here is to sample x0 conditioned on x1, then x1 conditioned on x0. The model as written below works because of the symmetry of the Gaussian likelihood function under the exchange of x (the data) and mu, when a single data point is observed. What we really want to do is set mu to 0 and then draw samples x from this distribution. But there is no way to do that directly in Venture that I can think of. Instead, we fix a single observation of x to 0 and sample values of mu."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_model1(rho):\n",
      "    print 'Building model 1'\n",
      "    ripl = make_lite_church_prime_ripl()\n",
      "    program = '''\n",
      "    [assume rho {0}]\n",
      "    [assume x0 (scope_include (quote data) 0 (uniform_continuous -4 4))]\n",
      "    [assume x1 (scope_include (quote data) 1 (uniform_continuous -4 4))]\n",
      "    [assume out (multivariate_normal (array x0 x1) (matrix (list (list 1 rho) (list rho 1))))]\n",
      "    '''.format(rho)\n",
      "    _ = ripl.execute_program(program)\n",
      "    v = [{\"type\": \"real\", \"value\": 0}, {\"type\": \"real\", \"value\": 0}]\n",
      "    ripl.observe(\"out\", {\"type\":\"list\",\"value\":v})\n",
      "    ripl.force('x0', -1.0)\n",
      "    ripl.force('x1', -1.0)\n",
      "    return ripl\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Second approach - write the log density explicitly using a custom SP"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "More explicit conceptually, but requires a bit more code. Write a random PSP whose output we never use - instead, we use it only the evaluate the likelihood of the input values given a covariance we pass in on construction. \n",
      "Performing inference effectively in this setting is enabled by Venture scopes; we just only perform inference in the scope of the input values and ignore the simulations from the helper SP."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.lite.utils import logDensityMVNormal\n",
      "from venture.lite.psp import RandomPSP\n",
      "from venture.lite.builtin import typed_nr\n",
      "import random\n",
      "import venture.lite.value as v\n",
      "import numpy as np\n",
      "from numpy import linalg as npla"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class BivariateNormalPotential(RandomPSP):\n",
      "    def __init__(self, rho):\n",
      "        self.mu = np.array([0,0])\n",
      "        self.Sigma = np.array([[1, rho], [rho, 1]])\n",
      "\n",
      "    def simulate(self, args):\n",
      "        # Doesn't matter.\n",
      "        return random.random()\n",
      "\n",
      "    def logDensity(self, x, args):\n",
      "        # x doesn't matter, the inputs do.\n",
      "        return logDensityMVNormal(self.__parse_args__(args), self.mu, self.Sigma)\n",
      "\n",
      "    def gradientOfLogDensity(self, x, args):\n",
      "        # gradient with respect to x 0\n",
      "        # gradient with respect to args is what we care about\n",
      "        (mu, sigma) = self.mu, self.Sigma\n",
      "        isigma = npla.inv(sigma)\n",
      "        actual_x = self.__parse_args__(args)\n",
      "        gradX = 0\n",
      "        actual_gradX = -np.dot(isigma, np.transpose(actual_x-mu))\n",
      "        return gradX, [actual_gradX[0], actual_gradX[1]]\n",
      "\n",
      "    def description(self,name):\n",
      "        return \"Simulates a Gaussian potential function.\" % name\n",
      "\n",
      "    def __parse_args__(self, args):\n",
      "        return np.array([args.operandValues[0], args.operandValues[1]])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def build_model2(rho):\n",
      "    print 'Building model 2'\n",
      "    ripl = make_lite_church_prime_ripl()\n",
      "    normalpotential_sp = typed_nr(BivariateNormalPotential(rho), [v.NumberType(), v.NumberType()], v.NumberType())\n",
      "    ripl.bind_foreign_sp('normal_potential', normalpotential_sp)\n",
      "    program = '''\n",
      "    [assume x0 (scope_include (quote data) 0 (uniform_continuous -4 4))]\n",
      "    [assume x1 (scope_include (quote data) 1 (uniform_continuous -4 4))]\n",
      "    [assume helper (normal_potential x0 x1)]\n",
      "    '''\n",
      "    _ = ripl.execute_program(program)\n",
      "    ripl.force('x0', -1.0)\n",
      "    ripl.force('x1', -1.0)\n",
      "    return ripl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Code to wrap the models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def annotate_plotf(plotf_output, elapsed, rho):\n",
      "    'Make the plotf output prettier'\n",
      "    fig = plotf_output.draw()[0]\n",
      "    ax = fig.axes[0]\n",
      "    ax.set_title('Elapsed time: {0:0.2f}s'.format(elapsed))\n",
      "    bivariate_normal_contour(ax, mu = np.array([0,0]),\n",
      "                           Sigma = np.array([[1, rho], [rho, 1]]),\n",
      "                           colors = ['black'])\n",
      "    ax.set_xlim([-3,3])\n",
      "    ax.set_ylim([-3,3])\n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_experiment(infer, niter, rho, model):\n",
      "    builder = build_model1 if model == 1 else build_model2\n",
      "    r = builder(rho)\n",
      "    cycle_cmd = '(cycle ((plotf p0d1ds x0 x1) {0}) {1})'.format(infer, niter)\n",
      "    start = time()\n",
      "    res = r.infer(cycle_cmd)\n",
      "    elapsed = time() - start\n",
      "    fig = annotate_plotf(res, elapsed, rho)\n",
      "    return fig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Reality check: if the two different model specifications truly encode the same potential, running Nesterov from the same starting point should produce identical results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) all 0.1 10 1)', \n",
      "                     30, 0.99, model = 1)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) all 0.1 10 1)', \n",
      "                     30, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) 0 0.02 10 1) (nesterov (quote data) 1 0.02 10 1)', \n",
      "                     50, 0.99, model = 1)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) 0 0.02 10 1) (nesterov (quote data) 1 0.02 10 1)', \n",
      "                     50, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The results seem identical. The second model is actually a bit faster, I think because the program is smaller. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Final check that they do the same thing: HMC, which incorporates randomness and gradients"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(hmc (quote data) 0 0.1 10 1) (hmc (quote data) 1 0.1 10 1)', \n",
      "                     70, 0.99, model = 1)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(hmc (quote data) 0 0.1 10 1) (hmc (quote data) 1 0.1 10 1)', \n",
      "                     70, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the second model is a bit faster I'll use that.\n",
      "Single-site MH does OK sometimes, and very poorly on others. Although we see random-walk behavior, it is for a very different reason than in the perhaps more \"conventional\" MH with a Gaussian proposal centered on the current state. Here, we always resample from the prior, but the heavy correlation between variables precludes large moves through the state space."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Comparing inference methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can look at how the different inference methods do for the model. I'll use the second specification because it's a bit faster."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Venture MH"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(mh (quote data) 0 1) (mh (quote data) 1 1)', 200, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If the variables are uncorrelated, Venture MH does quite well"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(mh (quote data) 0 1) (mh (quote data) 1 1)', 200, 0, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Slice sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Seems to be slightly more efficient than MH."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(slice (quote data) 0 1 100 1) (slice (quote data) 1 1 100 1)', 90, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As with MH, uncorrelated is easy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(slice (quote data) 0 1 100 1) (slice (quote data) 1 1 100 1)', 90, 0.0, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Making a bad guess on the length scale for slice sampling, however, slows sampling down substantially - both wall time and exploration rate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(slice (quote data) 0 0.01 100 1) (slice (quote data) 1 0.01 100 1)', 80, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The performance of slice sampling with \"doubling\" instead of \"stepping out\" seems roughly comparable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(slice_doubling (quote data) 0 1 100 1) (slice_doubling (quote data) 1 1 100 1)', \n",
      "                     70, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "HMC seems to do pretty nicely"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Does very well..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(hmc (quote data) 0 0.1 10 1) (hmc (quote data) 1 0.1 10 1)', \n",
      "                     70, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "...as long as you get the parameters right. Changing the learning rate from 0.1 to 0.3 totally wrecks things."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(hmc (quote data) 0 0.3 10 1) (hmc (quote data) 1 0.3 10 1)', 70, 0.99,\n",
      "                     model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Proposing to both variables at once seems more efficient, since it allows you to climb the gradient directly"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(hmc (quote data) all 0.1 10 1)', 70, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gradient methods"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradient ascent is effective and finds the maximum immediately if you pick the correct learning rate; Nesterov and MAP perform similarly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) all 1 10 1)', 2, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(map (quote data) all 1 10 1)', 2, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Picking the wrong learning rate can really slow things down."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) all 0.01 10 1)', 30, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Coordinate ascent is substantially slower and more fickle. Pick the learning rate right and you get gradual convergence."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) 0 0.02 10 1) (nesterov (quote data) 1 0.02 10 1)', \n",
      "                     50, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Guess a little bit wrong on the learning rate and you get a big mess. For highly correlated distributions, seems safer to climb the gradient instead of making coordinatewise updates."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(nesterov (quote data) 0 0.03 10 1) (nesterov (quote data) 1 0.03 10 1)', 50, 0.99,\n",
      "                     model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Variational approximation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Mean field methods run a lot of iterations very quickly, but seem to get \"stuck\" for long periods. I have a hunch that the behavior here could depend on the learning rate, which currently is not exposed to the user."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(meanfield (quote data) all 200 1)', 300, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using alternatively proposals is substantially slower, but the sampler tends to get stuck less."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = run_experiment('(meanfield (quote data) 0 100 1) (meanfield (quote data) 1 100 1)', \n",
      "                     200, 0.99, model = 2)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}