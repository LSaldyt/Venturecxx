<% sample_size = 500 %>
<% if ENV['SMOKETEST'] then sample_size = 5 end %>
<% smaller_sample_size = [sample_size / 5, 5].max %>
<% refman_url = "http://probcomp.csail.mit.edu/venture/edge/reference/" %>

[Next: TODO idk >](../idk/index.html)

Getting Started
===============

As a reminder, you can start an interactive Venture session with

```
$ venture lite
```

You can also run a file such as `script.vnts` using

```
$ venture lite -f script.vnts
```

A Simple Mixture Model
======================

One simple way to define a mixture model is to choose a mixture component at
top-level, and define each component in separate code path.

```venture
assume components = array(
    proc() { uniform_continuous(0, 10) },  // component 0
    proc() { uniform_continuous(0, 5) },   // component 1
    proc() { uniform_continuous(0, 2) }    // component 2
    );

assume mweights = simplex(0.3, 0.3, 0.4);

assume cnum = tag(quote(cnum), 0, categorical(mweights));
assume f = lookup(components, cnum);

sample f()
```

Here the value of `f()` is "doubly random" -- the procedure `f` is randomly
chosen, and after `f` is fixed, the output of a call to `f` is also random.
Equivalently, the value of `f()` follows a mixture model with mixture
components `components` and corresponding weights `mweights`.  

It can be instructive to examine the effect of observations.  The prior on
mixture components is:

```church'
(plot 'h0
 (run
  (accumulate_dataset <%= smaller_sample_size %>
   (do (mh 'cnum one 10)
       (collect cnum)))))
```

Now let's make some observations

```venture
observe f() = 3.3;
observe f() = 3.1;
observe f() = 4.5;
infer incorporate();
```

and examine the posterior

```church'
(plot 'h0
 (run
  (accumulate_dataset <%= smaller_sample_size %>
   (do (mh 'cnum one 10)
       (collect cnum)))))
```

As we would expect, the possibility of `uniform_continuous(0, 2)` has been
eliminated, as the sampled values of `f()` are not possible in that component.
The remaining components are `uniform_continuous(0, 10)` and
`uniform_continuous(0, 5)`.

Notice that `uniform_continuous(0, 10)` is much less probable than
`uniform_continuous(0, 5)` in the posterior, whereas it was equally probable in
the prior.  This is a case of the _Bayesian Occam's razor:_ Bayesian inference
"automatically" favors parsimonious models.  Said differently, in the presence
of data, the likelihood will penalize models with overly broad explanatory
power.  In the present case this shows up as: if `f` were
`uniform_continuous(0, 10)`, wouldn't it be a suspicious coincidence that three
independent samples of `f()` were all between 3 and 5?  The coincidence would
be much less suspicious if `f` were `uniform_continuous(0, 5)`.

Regression With a Mixture
=========================
Taking the above one step further, we can use a mixture model to perform
regression.  The following code asks, "what is the posterior on models that are
either linear or sine wave, conditioned on the given data?"

```venture
clear;
assume a = uniform_continuous(0-2, 2);
assume b = uniform_continuous(0-2, 2);
assume c = uniform_continuous(0-2, 2);
assume d = uniform_continuous(0, 3);
assume e = uniform_continuous(0, 2*3.14159);
assume components = array(
    proc(x) { a*x + b },
    proc(x) { c*sin(d*x + e) }
    );
assume mweights = simplex(0.5, 0.5);
assume f = categorical(mweights, components);

assume v = 0.1;
define obs_f = proc(xy) {
    run(do(
        x <- return(lookup(xy, 0)),
        y <- return(lookup(xy, 1)),
        observe(normal(f(unquote(x)), v), y)))
};
define xs = array(9.9, 9.07, 2.17, 3.76, 8.09, 8.72, 9.04, 5.9, 9.34, 1.14, 1.59, 3.27, 3.63, 2.11, 5.3, 6.08, 2.83, 3.88, 0.58, 1.9, 6.54, 8.28, 9.58, 9.98, 1.66, 3.9, 3.36, 5.76, 1.57, 3.92);
define ys = array(0-0.67, 0.51, 0-0.69, 1.04, 0.87, 1.0, 0.6, 0-1.09, 0.06, 0-0.88, 0-1.01, 0.7, 1.05, 0-0.87, 0-0.52, 0-1.16, 0.11, 1.05, 0.0, 0-0.86, 0-0.92, 1.18, 0-0.02, 0-0.91, 0-1.13, 0.99, 1.01, 0-1.0, 0-1.17, 1.13);
define data = zip(xs, ys);
infer return(mapv(obs_f, data));
infer incorporate();

infer default_markov_chain(1000);
// TODO collect the value of mapv(f, linspace(0, 10, 50)) as inference is done (say, 100 steps at a time) and plot them as superimposed line graphs with color proportional to time
```

Another interesting case of mixture regression is when the mixture components
are models of increasing power, where more powerful models are harder to
tune/fit to the data.  In that case mixture regression can be viewed as a way
of training all the components in parallel, and a sample of the computed
(approximate) posterior consists of a choice of model along with values of the
parameters for that model.

```venture
clear;
assume r = 0 - beta(2,2);
assume components = array(
    proc(x) { 0.3 },           // component 0
    proc(x) { pow(x, 0-1) },   // component 1
    proc(x) { pow(x, r) }      // component 2
    );

assume mweights = simplex(0.3, 0.3, 0.4);
assume cnum = tag(quote(cnum), 0, categorical(mweights));
assume f = lookup(components, cnum);

assume v = 0.05;
define obs_f = proc(xy) {
    run(do(
        x <- return(lookup(xy, 0)),
        y <- return(lookup(xy, 1)),
        observe(normal(f(unquote(x)), v), y)))
};
define xs = array(3.54, 4.38, 4.72, 2.06, 2.91, 1.83, 0.82, 3.92, 3.46, 3.83, 0.44, 3.46, 3.69, 3.34, 0.56, 1.61, 4.43, 2.98, 2.94, 1.6);
define ys = array(1.03, 0.68, 0.29, 0-1.03, 0.41, 0-1.13, 0-0.55, 1.29, 1.11, 1.1, 0-0.08, 0.87, 1.11, 0.9, 0-0.11, 0-1.02, 0.87, 0.23, 0.37, 0-1.08);
define data = zip(xs, ys);
infer return(mapv(obs_f, data));
infer incorporate();

infer default_markov_chain(1000);

// TODO plot cnum vs time and r vs time, as two superimposed line graphs.  The data was generated with r=0.6, can change that to use an r that makes it take longer to decide, if that looks better.
```

TODO There is a lesson about Bayesian something-or-other here, where the
posterior weight of a component is the average goodness of randomly drawn
parameters, not just the goodness of the MAP (though for some setups those can
end up being approximately the same thing).  Not sure whether this is something
I want to try to communicate though.  Actually, this lesson was probably
already taught in the similar example of linear-or-quadratic regression.  Maybe
I don't even need the above, idk.

GP Regression
=============
While the above examples rergess nonlinearly on a data set, they are still
hard-coded parametric models, and as such, the amount of modeller/programmer
resources required to write them grows with the complexity of the model.

We will now look at Gaussian process models, which "let the data speak for
itself" in the sense that the "effective dimensionality" of the model grows
with the number of data points.  For an introduction to Gaussian processes
viewed as random functions, see Section 2.2 of [Rasmussen and
Williams](http://www.gaussianprocess.org/gpml/chapters/RW2.pdf).

```venture
clear;
run(load_plugin("gpexample_plugin.py"));
assume se = make_squaredexp(1.2, 0.8);
assume zero_func = make_const_func(0.0);
assume g = make_gp(zero_func, se);
```

The Gaussian process `g` can be thought of as a random function whose prior is
a Gaussian process with mean zero and covariance function `se`, that is,
squared exponential:

```
prior_covariance(g(x1), g(x2)) = 1.2 * exp((x1-x2)^2 / (2*0.8))
```

We can observe the value of `g` at given data points:

```venture
assume xs = array(3.2, 2.2, 8.7);
define ys = array(8.7, 0.2, 5.5);
run(observe(g(xs), ys));
infer incorporate();
// TODO plot, I dunno, 30 samples of mapv(g, xs) where xs = linspace(0, 10, 50), as 30 superimposed line graphs (so the set of (x,y) pairs for one line is zip(xs, mapv(g, xs))).  The samples do not have to come from different particles.
```

The posterior of `g` is again a Gaussian process, with a different mean
function and a different covariance function (see
[R&W](http://www.gaussianprocess.org/gpml/chapters/RW2.pdf) for more on this).
The inputs and outputs of `g` are arrays, because `g` samples from the joint
distribution of the GP's values on all supplied inputs.

Exercise: verify and explain why the following two commands are not equivalent:

```venture
assume gg = proc(x) { lookup(g(array(x)), 0) };
sample array(gg(1.0), gg(2.0));
sample g(array(1.0, 2.0));
```

Note that the choice of covariance function can itself be random (as long as
all possible functions are symmetric and positive-semidefinite).  For example,
rather than defining `se = make_squaredexp(1.2, 0.8)`, we could have defined
`se = make_squaredexp(sigma_prior(), l_prior())`, where `sigma_prior` and
`l_prior` are stochastic procedures.  The inference engine could then be used
to perform parameter estimation; in fact, we do this below, in the section on
Bayesian optimization.  It would even be possible to infer the structure of the
covariance function, if we had in mind a generative model of covariance
function structures.

Structure
---------
TODO write exposition

```venture
assume twoterms = tag(quote(structure), 0, flip());
assume singleton_cov = tag(quote(structure), 1, uniform_discrete(0,2));
assume pair_cov_1 = tag(quote(structure), 2, uniform_discrete(0,2));
assume pair_cov_2 = tag(quote(structure), 3, uniform_discrete(0,2));
assume pair_cov_op = tag(quote(structure), 4, uniform_discrete(0,2));

assume covs = array(make_linear_cov, make_periodic_cov);
assume ops = array(add_funcs, mult_funcs);
assume paramgens = list(
    proc() { array(
        tag(quote(param), 0, uniform_continuous(0-2, 2))  // sf for LIN
        )
    },
    proc() { array(
        tag(quote(param), 0, uniform_continuous(0-2, 2)), // l for PER
        tag(quote(param), 0, uniform_continuous(0-2, 2)), // p for PER
        tag(quote(param), 0, uniform_continuous(0-2, 2))  // sf for PER
        )
    });

assume cov = if (twoterms) {
    lookup(covs, singleton_cov)
} else {
    lookup(ops, pair_cov_op)(
        apply(lookup(covs, pair_cov_1), lookup(paramgens, pair_cov_1)()),
        apply(lookup(covs, pair_cov_2), lookup(paramgens, pair_cov_2)()))
};
```

gpmem
=====
We recently developed language support for a new idiom for GP modelling, called
_Gaussian process memoization_, or `gpmem`.  The idea is to wrap a function `f`
into a package containing both a self-caching "prober" (which actually calls
`f`) and a statistical emulator, which simulates `f` using a GP prior
conditioned on the values of all previous invocations of the prober.  The
arguments to `gpmem` are the function `f`, a prior mean function for the
GP-based emulator, and a prior covariance function.  For example:

```venture
assume f = make_audited_expensive_function("f");
assume package = gpmem(f, zero_func, se);

assume ffive = first(package)(5);
assume fsix = first(package)(6);

sample second(package)(array(5.5))
```

Note that the emulator expects an array as input (since it is a GP, as above),
but the prober does not (since `f` is just a function, there is no need to
"jointly" evaluate `f` at multiple points).

Our original motivation for this idiom was the case when `f` is too expensive
to compute many times, so we need to learn using a limited number of samples.
Later we realized that GP regression in general can be cast as a special case
of `gpmem`: regression on a dataset `D` (with keys `x` and values `y = D[x]`)
is the same as statistical emulation of the function `f` defined in pseudocode
as

```
def f(x):
  if x in D:
    return D[x]
  else:
    raise Exception("Illegal input")
```

Observing data points {`(x, y)`} is the same as probing the value of `f` at all
the `x`s.

If you can think of any other potential applications of `gpmem`, we would love
to hear them.

Bayesian Optimization Example
-----------------------------
The example below showcases several nice features of `gpmem` and GP modelling
in Venture.  In the code below we aim to find the maximum value of our
expensive function `V` on the interval [-20, 20] using as few evaluations of
`V` as possible.  We do this by wrapping `V` in a `gpmem` and repeatedly using
the GP-based emulator to find a point where we have the most evidence than `V`
will be large.  We probe `V` at said point, which gives our GP model more
information to choose the next probe point.  In addition, we perform
Metropolis-Hastings inference on the hyperparameters of the covariance function
after each probe.

```venture
assume V = make_audited_expensive_function("V");

assume V_sf = tag(quote(hyper), 0, uniform_continuous(0, 10));
assume V_l = tag(quote(hyper), 1, uniform_continuous(0, 10));
assume V_se = make_squaredexp(V_sf, V_l);
assume V_package = gpmem(V, zero_func, V_se);

// A very naive estimate of the argmax of the given function
define mc_argmax = proc(func) {
  run(do(
    candidate_xs <- return(mapv(proc(i) {uniform_continuous(0-20, 20)},
                          linspace(0, 19, 20))),
    candidate_ys <- return(mapv(func, candidate_xs)),
    return(lookup(candidate_xs,
                  argmax_of_array(candidate_ys)))))
};

// Shortcut to sample the emulator at a single point without packing
// and unpacking arrays
define V_emu_pointwise = proc(x) {
    run(sample(lookup(second(V_package)(array(unquote(x))),
                      0)))
};

infer repeat(15, do(pass,
    // Probe V at the point mc_argmax(V_emu_pointwise)
    predict(first(V_package)(unquote(mc_argmax(V_emu_pointwise)))),
    // Infer hyperparameters
    mh(quote(hyper), one, 50)));
```

Here is a visualization of a run of this program (not the same run as above),
showing the state of the emulator at various times.   Each row adds a new probe
point.  The left column is before hyperparameter inference, the right column
after.  The blue curve is the true function `V`, the green circle is the next
chosen probe point, and the purple square is the best probe point found so far.
Notice that probes tend to be chosen either where the value of the emulator is
high, or where the emulator has high uncertainty.  Also note that the "crazy"
plots on the third-fourth rows correspond to a "crazy" choice of parameters.
This is not a mistake -- Metropolis-Hastings samplers will make crazy choices
every once in a while (as will the true posterior, with some small
probability).

![Visualization of Bayesian optimization](BayesOpt_gpmem_sequence.png)

[Next: TODO idk >](../idk/index.html)
