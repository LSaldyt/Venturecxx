{
 "metadata": {
  "name": "",
  "signature": "sha256:6ff1369b8fb01d183e3b93b1399a8a96f228fd2a80a967a824e302bb39352aae"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plan: once complete, we want to have all utils be importable so that later in tut we don't need to run all cells again. annoying to maintain this. for the friday tutorial, we can wait till all finished. then copy alll utils to a script. (maybe some way to import from a notebook?) best thing would be to have permanent version be in script and just print some of them in the notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduce Bag of Colored Balls Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model with no latents / state estimation\n",
      "\n",
      "def make_simple_bag_string(colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string= '''    \n",
      "    [assume hyper_alpha (array %s)]\n",
      "\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                                (dirichlet hyper_alpha) ) ) ]\n",
      "    ''' % prior\n",
      "    return string\n",
      "\n",
      "\n",
      "def data_observe(bag,color): \n",
      "    return ('(categorical (bag_prototype %i) )'%bag,'atom<%i>'%color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_data(bags,data):\n",
      "    for bag in range(bags):\n",
      "        print '\\nbag: ',bag\n",
      "        print 'colors: ', [el[1] for el in data if el[0]==bag]\n",
      "\n",
      "def make_even_data(bags,colors, draws_per_bag, max_alpha_prior):\n",
      "    data = [(bag,color) for bag in range(bags) for color in range(colors)] * draws_per_bag\n",
      "    params = { '(bag_prototype %i)'%bag:np.array( [1./colors]*colors )  for bag in range(bags)}\n",
      "    params['hyper_alpha'] = [max_alpha_prior]*colors\n",
      "    return data, params\n",
      "\n",
      "def make_conc_data(bags,colors, draws_per_bag, max_alpha_prior):\n",
      "    data = [(bag, np.mod(bag,colors)) for bag in range(bags)] * draws_per_bag\n",
      "    ptypes = [np.zeros(colors) for i in range(bags)]\n",
      "    for bag, zero in enumerate(ptypes):\n",
      "        zero[np.mod(bag,colors)] = 1\n",
      "    params = { '(bag_prototype %i)'%bag:ptype for bag,ptype in zip(range(bags),ptypes)}\n",
      "    params['hyper_alpha'] = [.01]*colors\n",
      "    return data, params\n",
      "\n",
      "def make_dataset(dataset,args):\n",
      "    return make_conc_data(*args) if dataset=='conc' else make_even_data(*args)\n",
      "\n",
      "\n",
      "def display_compare_queries(ripl, queries,gtruth_params, verbose=True):\n",
      "    inf_params = {}\n",
      "    \n",
      "    for q in queries: # print queries\n",
      "        if 'draw_bag' not in q and verbose:\n",
      "            print '%s    :'%q, np.round(ripl.sample(q),2)\n",
      "        inf_params[q] = ripl.sample(q)\n",
      "        \n",
      "    logscore = ripl.get_global_logscore()     \n",
      "    if verbose:\n",
      "        print '\\nLogscore: %.2f' % logscore\n",
      "    \n",
      "    # compare to gtruth\n",
      "    mse_ptypes = 0\n",
      "    mse_ar = lambda xs,ys: np.mean( (np.array(xs) - np.array(ys))**2 )\n",
      "    \n",
      "    for k in inf_params.keys():\n",
      "        \n",
      "        if k=='hyper_alpha':\n",
      "            mse_alpha = mse_ar(inf_params[k], gtruth_params[k])\n",
      "            if verbose:\n",
      "                print 'mse hyper:', mse_alpha\n",
      "        elif 'proto' in k:\n",
      "            mse_ptypes += mse_ar(inf_params[k], gtruth_params[k])\n",
      "    if verbose:\n",
      "        print 'mse ptypes:', mse_ptypes\n",
      "    \n",
      "    return dict(logscore=logscore, mse_alpha=mse_alpha, mse_ptypes=mse_ptypes)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model setup\n",
      "max_alpha_prior = 5\n",
      "bags, colors = 5,3\n",
      "v=make_ripl()\n",
      "simple_bag_model = make_simple_bag_string(colors)\n",
      "print 'Model: ',simple_bag_model,'\\n'\n",
      "v.execute_program(simple_bag_model)\n",
      "\n",
      "# data setup\n",
      "draws_per_bag = 5\n",
      "dataset = 'conc'\n",
      "data, gtruth_params = make_dataset(dataset,(bags, colors, draws_per_bag, max_alpha_prior))\n",
      "\n",
      "print '----\\nObserved draws for each bag: ' \n",
      "print_data(bags,data)\n",
      "\n",
      "observes = [data_observe(*datum) for datum in data]\n",
      "out = [v.observe(*observe) for observe in observes]\n",
      "queries = ['hyper_alpha'] + ['(bag_prototype %i)'%bag for bag in range(bags)]\n",
      "\n",
      "print '\\n\\n INFERENCE \\n --------- \\n Before inference: '\n",
      "display_compare_queries(v,queries,gtruth_params)\n",
      "v.infer(2000)\n",
      "print '\\n\\nAfter inference: '\n",
      "display_compare_queries(v,queries,gtruth_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model:      \n",
        "    [assume hyper_alpha (array (uniform_continuous 0.01 5) (uniform_continuous 0.01 5) (uniform_continuous 0.01 5))]\n",
        "\n",
        "    [assume bag_prototype (mem (lambda (bag)\n",
        "                                (dirichlet hyper_alpha) ) ) ]\n",
        "     \n",
        "\n",
        "----\n",
        "Observed draws for each bag: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "bag:  0\n",
        "colors:  [0, 0, 0, 0, 0]\n",
        "\n",
        "bag:  1\n",
        "colors:  [1, 1, 1, 1, 1]\n",
        "\n",
        "bag:  2\n",
        "colors:  [2, 2, 2, 2, 2]\n",
        "\n",
        "bag:  3\n",
        "colors:  [0, 0, 0, 0, 0]\n",
        "\n",
        "bag:  4\n",
        "colors:  [1, 1, 1, 1, 1]\n",
        "\n",
        "\n",
        " INFERENCE \n",
        " --------- \n",
        " Before inference: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hyper_alpha    : [ 2.29  3.07  1.77]\n",
        "(bag_prototype 0)    : [ 0.14  0.32  0.54]\n",
        "(bag_prototype 1)    : [ 0.31  0.53  0.16]\n",
        "(bag_prototype 2)    : [ 0.57  0.41  0.03]\n",
        "(bag_prototype 3)    : [ 0.38  0.54  0.07]\n",
        "(bag_prototype 4)    : [ 0.68  0.23  0.09]\n",
        "\n",
        "Logscore: -20.72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mse hyper: 5.89771381917\n",
        "mse ptypes: 1.55133506158\n",
        "\n",
        "\n",
        "After inference: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hyper_alpha    : [ 0.32  0.33  0.08]\n",
        "(bag_prototype 0)    : [ 0.85  0.    0.15]\n",
        "(bag_prototype 1)    : [ 0.21  0.79  0.  ]\n",
        "(bag_prototype 2)    : [ 0.    0.12  0.88]\n",
        "(bag_prototype 3)    : [ 0.87  0.13  0.  ]\n",
        "(bag_prototype 4)    : [ 0.01  0.99  0.  ]\n",
        "\n",
        "Logscore: 23.62\n",
        "mse hyper: 0.0686072739807\n",
        "mse ptypes: 0.0656901446927\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{'logscore': 23.61723508260917,\n",
        " 'mse_alpha': 0.068607273980699005,\n",
        " 'mse_ptypes': 0.06569014469270204}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Latents, inference without scopes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model with latents, still no scopes. show that inference doesn't do so well as we add latents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_latent_bag_string(bags,colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string=''' \n",
      "    [assume atom_number (lambda (atom) (+ 0 atom) ) ]\n",
      "    \n",
      "    [assume bags %i]\n",
      "    [assume hyper_alpha (array %s)]\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                                 (dirichlet hyper_alpha) ) ) ]\n",
      "\n",
      "    [assume draw_bag (mem (lambda (t)\n",
      "                            (atom_number\n",
      "                                (uniform_discrete 0 bags) ) ) )]\n",
      "\n",
      "    [assume t_color (mem (lambda (t) \n",
      "                           (categorical \n",
      "                             (bag_prototype (draw_bag t)) ) ) )]\n",
      "\n",
      "    ''' % (bags,prior)\n",
      "    return string\n",
      "\n",
      "def make_latent_bag_string_scopes(bags,colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string='''\n",
      "    [assume atom_number (lambda (atom) (+ 0 atom) ) ]\n",
      "\n",
      "    [assume bags %i]\n",
      "\n",
      "    [assume hyper_alpha (scope_include (quote hyper_alpha) 0\n",
      "                            (array %s) )]\n",
      "\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                               (scope_include (quote prototypes) bag\n",
      "                                   (dirichlet hyper_alpha) ) ) )]\n",
      "\n",
      "    [assume draw_bag (mem (lambda (t)\n",
      "                           (scope_include (quote latents) t\n",
      "                             (atom_number\n",
      "                               (uniform_discrete 0 bags) ) )) )]\n",
      "\n",
      "    [assume t_color (mem (lambda (t) \n",
      "                           (categorical \n",
      "                             (bag_prototype (draw_bag t) ) ) )) ]\n",
      "\n",
      "    '''%(bags,prior)\n",
      "    return string\n",
      "\n",
      "\n",
      "def data_observe(bag,color): \n",
      "    return ('(categorical (bag_prototype %i) )'%bag,'atom<%i>'%color)\n",
      "\n",
      "def data_latent_observe(t,color):\n",
      "    return ('(t_color %i)'%t, 'atom<%i>'%color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_latent_dataset(num_latents):\n",
      "    data = []\n",
      "    for t in range(num_latents):\n",
      "        color = 0 if (t <=.5*num_latents) else 1\n",
      "        data.append( (t,color) )\n",
      "    return data\n",
      "\n",
      "def load_model(bags,colors,make_string):\n",
      "    v = make_ripl()\n",
      "    v.execute_program( make_string(bags,colors) )\n",
      "    return v\n",
      "\n",
      "def load_observes(ripl, dataset, make_dataset_args, num_latents, verbose=False):\n",
      "    data1,gtruth_params = make_dataset(dataset, make_dataset_args )\n",
      "    data2 = make_latent_dataset( num_latents )\n",
      "    if verbose:\n",
      "        print '----\\nObserved draws for each bag: '; print_data(bags,data1)\n",
      "        print '\\n(t,color) for latent observes: ', data2\n",
      "\n",
      "    observes = [data_observe(*datum) for datum in data1] + [data_latent_observe(*datum) for datum in data2]\n",
      "    [ripl.observe(*observe) for observe in observes]\n",
      "    return gtruth_params\n",
      "\n",
      "def make_queries(bags,num_latents):\n",
      "    bag_queries = ['(bag_prototype %i)'%bag for bag in range(bags)]\n",
      "    latents = ['(draw_bag %i)'%t for t in range(num_latents)]\n",
      "    return ['hyper_alpha'] + bag_queries + latents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_infer_model(num_latents, verbosity=True):\n",
      "    \n",
      "    bags, colors = 4,4\n",
      "    draws_per_bag = 5\n",
      "    max_alpha_prior = 5\n",
      "\n",
      "    v = load_model(bags, colors, make_latent_bag_string)\n",
      "    \n",
      "    make_dataset_args = (bags, colors, draws_per_bag, max_alpha_prior)\n",
      "    gtruth_params = load_observes(v, dataset, make_dataset_args, num_latents, verbose=False)\n",
      "    queries = make_queries(bags,num_latents)\n",
      "\n",
      "    if verbosity:\n",
      "        print '\\n\\n INFERENCE \\n --------- \\n Before inference: '\n",
      "    display_compare_queries(v, queries,gtruth_params, verbose = False)\n",
      "    \n",
      "    start = time.time()\n",
      "    v.infer(num_transitions)\n",
      "    elapsed = time.time() - start\n",
      "    \n",
      "    if verbosity:\n",
      "        print 'elapsed: ',elapsed\n",
      "        print '\\n\\nAfter inference: '\n",
      "\n",
      "    result = display_compare_queries(v, queries, gtruth_params, verbose = verbosity)\n",
      "\n",
      "    return v, result\n",
      "\n",
      "\n",
      "def make_series(results,field='logscore'):\n",
      "    pairs = []\n",
      "    for k in sorted(results.keys()):\n",
      "        if isinstance(k,int):\n",
      "            pairs.append( (k, results[k][field] ) )\n",
      "    return pairs\n",
      "   \n",
      "    \n",
      "num_latents_values = range(1,3000,900)\n",
      "num_transitions = 100\n",
      "dataset = 'conc'\n",
      "\n",
      "verbosity = False\n",
      "results = dict(num_transitions = num_transitions)\n",
      "\n",
      "for i,num_latents in enumerate(num_latents_values):\n",
      "    if verbosity or True:\n",
      "        print 'RUN: %i' % i\n",
      "    tic = time.time()\n",
      "    v, result = load_infer_model(num_latents, verbosity=verbosity)\n",
      "    print 'Time ', time.time() - tic\n",
      "    results[num_latents] = result\n",
      "    \n",
      "series_alpha = make_series(results,'mse_alpha')\n",
      "series_ptypes = make_series(results,'mse_ptypes')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RUN: 0\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.7569668293\n",
        "RUN: 1\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.03855490685\n",
        "RUN: 2\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 15.1592781544\n",
        "RUN: 3\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 22.9759550095\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots()\n",
      "ax.scatter(*zip(*series_alpha),label='hyper_alpha mse')\n",
      "ax.scatter(*zip(*series_ptypes),c='m',marker='+', label='ptypes mse')\n",
      "ax.set_xlabel('num_latents')\n",
      "ax.set_ylabel('MSE')\n",
      "ax.legend()\n",
      "\n",
      "double_plot = False\n",
      "\n",
      "if double_plot:\n",
      "    fig,ax = plt.subplots(1,2)\n",
      "    ax[0].scatter(*zip(*series_alpha),label='hyper_alpha mse')\n",
      "    ax[1].scatter(*zip(*series_ptypes),c='m',marker='+', label='ptypes mse')\n",
      "    ax.set_xlabel('num_latents')\n",
      "    ax.set_ylabel('MSE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAENCAYAAADgwHn9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0xc9cefcc>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "basics of scopes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simplified version of hierarchical bags model\n",
      "\n",
      "from venture.venturemagics.ip_parallel import *\n",
      "model='''\n",
      "[assume hyper_alpha (scope_include  (quote hyper_alpha)  0 \n",
      "                      (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)) ) ]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet hyper_alpha) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 1\n",
      "               (dirichlet hyper_alpha) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loop over repetitions of infer_prog, tracking changes \n",
      "def loop_infer(infer_prog,limit=5):\n",
      "    print 'i: hyper_alpha,  bag0  bag1'\n",
      "    for i in range(5):\n",
      "        alpha = v.sample('hyper_alpha') \n",
      "        bags_0 = [v.sample('bag%i'%j)[0] for j in (0,1)]\n",
      "        print '%i: %.2f %.2f ,  %.2f   %.2f'%(i,alpha[0],alpha[1],bags_0[0],bags_0[1])\n",
      "        v.infer( infer_prog )\n",
      "\n",
      "# default scope: all vars change        \n",
      "v = mk_p_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh default one 10)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 2.85 5.89 ,  0.57   0.22\n",
        "1: 2.52 4.82 ,  0.61   0.25\n",
        "2: 2.88 2.74 ,  0.61   0.21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 0.91 2.03 ,  0.61   0.27\n",
        "4: 2.18 2.03 ,  0.80   0.24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# block only contains bag0\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 1.96 5.60 ,  0.16   0.39\n",
        "1: 1.96 5.60 ,  0.39   0.39\n",
        "2: 1.96 5.60 ,  0.61   0.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.96 5.60 ,  0.47   0.39\n",
        "4: 1.96 5.60 ,  0.42   0.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample random member of scope\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes one 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 3.76 4.43 ,  0.43   0.28\n",
        "1: 3.76 4.43 ,  0.78   0.14\n",
        "2: 3.76 4.43 ,  0.78   0.61\n",
        "3: 3.76 4.43 ,  0.78   0.31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 3.76 4.43 ,  0.78   0.18\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample all members as scope (blocked proposal)\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes all 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 4.08 2.76 ,  0.77   0.69\n",
        "1: 4.08 2.76 ,  0.45   0.47\n",
        "2: 4.08 2.76 ,  0.44   0.56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 4.08 2.76 ,  0.73   0.61\n",
        "4: 4.08 2.76 ,  0.82   0.69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh hyper_alpha all 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 5.78 2.86 ,  0.58   0.88\n",
        "1: 5.84 1.58 ,  0.58   0.88\n",
        "2: 6.47 2.15 ,  0.58   0.88\n",
        "3: 6.20 3.67 ,  0.58   0.88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 6.20 3.67 ,  0.58   0.88\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(cycle ( (mh prototypes one 5) (mh hyper_alpha one 5) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 1.84 4.07 ,  0.12   0.32\n",
        "1: 2.31 6.32 ,  0.37   0.10\n",
        "2: 2.45 3.06 ,  0.56   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.27 3.51 ,  0.36   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.27 3.51 ,  0.36   0.19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vl=mk_l_ripl()\n",
      "vl.execute_program(model)\n",
      "loop_infer( '(cycle ( (rejection prototypes one 5) (rejection hyper_alpha one 5) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 1.27 3.51 ,  0.46   0.18\n",
        "1: 1.64 6.78 ,  0.20   0.28\n",
        "2: 1.48 6.91 ,  0.20   0.23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.48 6.91 ,  0.20   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.48 6.91 ,  0.34   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(cycle ( (func_pgibbs prototypes one 10 3) (func_pgibbs hyper_alpha one 10 3) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 5.24 0.32 ,  0.99   0.60\n",
        "1: 2.96 1.07 ,  0.99   0.79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2: 1.81 0.16 ,  0.99   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 2.31 0.61 ,  1.00   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 2.31 0.61 ,  1.00   0.23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# slice kernel needs unbounded support\n",
      "# egibbs needs finite, discrete support \n",
      "# hmc need gradients\n",
      "\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(cycle ( (func_pgibbs prototypes one 5) (mh hyper_alpha one 1) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import *\n",
      "\n",
      "# show dynamic scope. hyper_alpha goes in prototypes scope\n",
      "# NOTE issue of inconsistent blocks\n",
      "\n",
      "model='''\n",
      "[assume hyper_alpha (mem ( lambda () (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)))) ]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''\n",
      "def loop_infer(infer_prog,limit=5):\n",
      "    print 'i: hyper_alpha,  bag0  bag1'\n",
      "    for i in range(5):\n",
      "        alpha = v.sample('(hyper_alpha)') \n",
      "        bags_0 = [v.sample('bag%i'%j)[0] for j in (0,1)]\n",
      "        print '%i: %.2f %.2f ,  %.2f   %.2f'%(i,alpha[0],alpha[1],bags_0[0],bags_0[1])\n",
      "        v.infer( infer_prog )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v=mk_p_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 6.90 0.47 ,  0.98   0.52\n",
        "1: 0.20 0.16 ,  0.99   0.07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2: 1.21 1.00 ,  0.80   0.34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.31 2.19 ,  0.48   0.49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 3.27 2.80 ,  0.52   0.29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#whats going on here?\n",
      "model='''\n",
      "[assume hyper_alpha (mem ( lambda ()\n",
      "                            (scope_include (quote hyper_alpha) 0\n",
      "                              (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)))) )]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 2.59 4.26 ,  0.10   0.33\n",
        "1: 3.08 1.23 ,  0.63   0.63\n",
        "2: 1.99 2.29 ,  0.52   0.38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 2.42 2.24 ,  0.93   0.17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.13 0.36 ,  0.98   0.58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scopes for Latents MOdel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "use mix and pgibbs kernels as in scopes.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model setup\n",
      "bags, colors = 5,3\n",
      "\n",
      "# data setup\n",
      "draws_per_bag = 5\n",
      "dataset = 'conc'\n",
      "num_latents = 16\n",
      "\n",
      "v = load_model(bags, colors, make_latent_bag_string_scopes)\n",
      "load_observes(v, bags, colors, draws_per_bag, num_latents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'load_model' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-a3fd61e0a1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_latents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_latent_bag_string_scopes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mload_observes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraws_per_bag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_latents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "queries = make_queries(bags,num_latents)\n",
      "print 'Before inference: '\n",
      "print_queries(queries)\n",
      "v.infer(5000)\n",
      "print '\\n\\nAfter inference: '\n",
      "print_queries(queries)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}