{
 "metadata": {
  "name": "",
  "signature": "sha256:86c7b51f22eeb3905f4e6693cf3560f973fedc6c82f591d0041bf3e5e2d7b173"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plan: once complete, we want to have all utils be importable so that later in tut we don't need to run all cells again. annoying to maintain this. for the friday tutorial, we can wait till all finished. then copy alll utils to a script. (maybe some way to import from a notebook?) best thing would be to have permanent version be in script and just print some of them in the notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduce Bag of Colored Balls Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model with no latents / state estimation\n",
      "\n",
      "def make_simple_bag_string(colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string= '''    \n",
      "    [assume hyper_alpha (array %s)]\n",
      "\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                                (dirichlet hyper_alpha) ) ) ]\n",
      "    ''' % prior\n",
      "    return string\n",
      "\n",
      "\n",
      "def data_observe(bag,color): \n",
      "    return ('(categorical (bag_prototype %i) )'%bag,'atom<%i>'%color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_data(bags,data):\n",
      "    for bag in range(bags):\n",
      "        print '\\nbag: ',bag\n",
      "        print 'colors: ', [el[1] for el in data if el[0]==bag]\n",
      "\n",
      "def make_even_data(bags,colors, draws_per_bag, max_alpha_prior):\n",
      "    data = [(bag,color) for bag in range(bags) for color in range(colors)] * draws_per_bag\n",
      "    params = { '(bag_prototype %i)'%bag:np.array( [1./colors]*colors )  for bag in range(bags)}\n",
      "    params['hyper_alpha'] = [max_alpha_prior]*colors\n",
      "    return data, params\n",
      "\n",
      "def make_conc_data(bags,colors, draws_per_bag, max_alpha_prior):\n",
      "    data = [(bag, np.mod(bag,colors)) for bag in range(bags)] * draws_per_bag\n",
      "    ptypes = [np.zeros(colors) for i in range(bags)]\n",
      "    for bag, zero in enumerate(ptypes):\n",
      "        zero[np.mod(bag,colors)] = 1\n",
      "    params = { '(bag_prototype %i)'%bag:ptype for bag,ptype in zip(range(bags),ptypes)}\n",
      "    params['hyper_alpha'] = [.01]*colors\n",
      "    return data, params\n",
      "\n",
      "def make_dataset(dataset,args):\n",
      "    return make_conc_data(*args) if dataset=='conc' else make_even_data(*args)\n",
      "\n",
      "\n",
      "def display_compare_queries(ripl, queries,gtruth_params, verbose=True):\n",
      "    inf_params = {}\n",
      "    \n",
      "    for q in queries: # print queries\n",
      "        if 'draw_bag' not in q and verbose:\n",
      "            print '%s    :'%q, np.round(ripl.sample(q),2)\n",
      "        inf_params[q] = ripl.sample(q)\n",
      "        \n",
      "    logscore = ripl.get_global_logscore()     \n",
      "    if verbose:\n",
      "        print '\\nLogscore: %.2f' % logscore\n",
      "    \n",
      "    # compare to gtruth\n",
      "    mse_ptypes = 0\n",
      "    mse_ar = lambda xs,ys: np.mean( (np.array(xs) - np.array(ys))**2 )\n",
      "    \n",
      "    for k in inf_params.keys():\n",
      "        \n",
      "        if k=='hyper_alpha':\n",
      "            mse_alpha = mse_ar(inf_params[k], gtruth_params[k])\n",
      "            if verbose:\n",
      "                print 'mse hyper:', mse_alpha\n",
      "        elif 'proto' in k:\n",
      "            mse_ptypes += mse_ar(inf_params[k], gtruth_params[k])\n",
      "    if verbose:\n",
      "        print 'mse ptypes:', mse_ptypes\n",
      "    \n",
      "    return dict(logscore=logscore, mse_alpha=mse_alpha, mse_ptypes=mse_ptypes)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model setup\n",
      "max_alpha_prior = 5\n",
      "bags, colors = 5,3\n",
      "v=make_ripl()\n",
      "simple_bag_model = make_simple_bag_string(colors)\n",
      "print 'Model: ',simple_bag_model,'\\n'\n",
      "v.execute_program(simple_bag_model)\n",
      "\n",
      "# data setup\n",
      "draws_per_bag = 5\n",
      "dataset = 'conc'\n",
      "data, gtruth_params = make_dataset(dataset,(bags, colors, draws_per_bag, max_alpha_prior))\n",
      "\n",
      "print '----\\nObserved draws for each bag: ' \n",
      "print_data(bags,data)\n",
      "\n",
      "observes = [data_observe(*datum) for datum in data]\n",
      "out = [v.observe(*observe) for observe in observes]\n",
      "queries = ['hyper_alpha'] + ['(bag_prototype %i)'%bag for bag in range(bags)]\n",
      "\n",
      "print '\\n\\n INFERENCE \\n --------- \\n Before inference: '\n",
      "display_compare_queries(v,queries,gtruth_params)\n",
      "v.infer(2000)\n",
      "print '\\n\\nAfter inference: '\n",
      "display_compare_queries(v,queries,gtruth_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model:      \n",
        "    [assume hyper_alpha (array (uniform_continuous 0.01 5) (uniform_continuous 0.01 5) (uniform_continuous 0.01 5))]\n",
        "\n",
        "    [assume bag_prototype (mem (lambda (bag)\n",
        "                                (dirichlet hyper_alpha) ) ) ]\n",
        "     \n",
        "\n",
        "----\n",
        "Observed draws for each bag: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "bag:  0\n",
        "colors:  [0, 0, 0, 0, 0]\n",
        "\n",
        "bag:  1\n",
        "colors:  [1, 1, 1, 1, 1]\n",
        "\n",
        "bag:  2\n",
        "colors:  [2, 2, 2, 2, 2]\n",
        "\n",
        "bag:  3\n",
        "colors:  [0, 0, 0, 0, 0]\n",
        "\n",
        "bag:  4\n",
        "colors:  [1, 1, 1, 1, 1]\n",
        "\n",
        "\n",
        " INFERENCE \n",
        " --------- \n",
        " Before inference: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hyper_alpha    : [ 2.29  3.07  1.77]\n",
        "(bag_prototype 0)    : [ 0.14  0.32  0.54]\n",
        "(bag_prototype 1)    : [ 0.31  0.53  0.16]\n",
        "(bag_prototype 2)    : [ 0.57  0.41  0.03]\n",
        "(bag_prototype 3)    : [ 0.38  0.54  0.07]\n",
        "(bag_prototype 4)    : [ 0.68  0.23  0.09]\n",
        "\n",
        "Logscore: -20.72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mse hyper: 5.89771381917\n",
        "mse ptypes: 1.55133506158\n",
        "\n",
        "\n",
        "After inference: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hyper_alpha    : [ 0.32  0.33  0.08]\n",
        "(bag_prototype 0)    : [ 0.85  0.    0.15]\n",
        "(bag_prototype 1)    : [ 0.21  0.79  0.  ]\n",
        "(bag_prototype 2)    : [ 0.    0.12  0.88]\n",
        "(bag_prototype 3)    : [ 0.87  0.13  0.  ]\n",
        "(bag_prototype 4)    : [ 0.01  0.99  0.  ]\n",
        "\n",
        "Logscore: 23.62\n",
        "mse hyper: 0.0686072739807\n",
        "mse ptypes: 0.0656901446927\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{'logscore': 23.61723508260917,\n",
        " 'mse_alpha': 0.068607273980699005,\n",
        " 'mse_ptypes': 0.06569014469270204}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Latents, inference without scopes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model with latents, still no scopes. show that inference doesn't do so well as we add latents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_latent_bag_string(bags,colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string=''' \n",
      "    [assume atom_number (lambda (atom) (+ 0 atom) ) ]\n",
      "    \n",
      "    [assume bags %i]\n",
      "    [assume hyper_alpha (array %s)]\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                                 (dirichlet hyper_alpha) ) ) ]\n",
      "\n",
      "    [assume draw_bag (mem (lambda (t)\n",
      "                            (atom_number\n",
      "                                (uniform_discrete 0 bags) ) ) )]\n",
      "\n",
      "    [assume t_color (mem (lambda (t) \n",
      "                           (categorical \n",
      "                             (bag_prototype (draw_bag t)) ) ) )]\n",
      "\n",
      "    ''' % (bags,prior)\n",
      "    return string\n",
      "\n",
      "def make_latent_bag_string_scopes(bags,colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string='''\n",
      "    [assume atom_number (lambda (atom) (+ 0 atom) ) ]\n",
      "\n",
      "    [assume bags %i]\n",
      "\n",
      "    [assume hyper_alpha (scope_include (quote hyper_alpha) 0\n",
      "                            (array %s) )]\n",
      "\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                               (scope_include (quote prototypes) bag\n",
      "                                   (dirichlet hyper_alpha) ) ) )]\n",
      "\n",
      "    [assume draw_bag (mem (lambda (t)\n",
      "                           (scope_include (quote latents) t\n",
      "                             (atom_number\n",
      "                               (uniform_discrete 0 bags) ) )) )]\n",
      "\n",
      "    [assume t_color (mem (lambda (t) \n",
      "                           (categorical \n",
      "                             (bag_prototype (draw_bag t) ) ) )) ]\n",
      "\n",
      "    '''%(bags,prior)\n",
      "    return string\n",
      "\n",
      "\n",
      "def data_observe(bag,color): \n",
      "    return ('(categorical (bag_prototype %i) )'%bag,'atom<%i>'%color)\n",
      "\n",
      "def data_latent_observe(t,color):\n",
      "    return ('(t_color %i)'%t, 'atom<%i>'%color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_latent_dataset(num_latents):\n",
      "    data = []\n",
      "    for t in range(num_latents):\n",
      "        color = 0 if (t <=.5*num_latents) else 1\n",
      "        data.append( (t,color) )\n",
      "    return data\n",
      "\n",
      "def load_model(bags,colors,make_string):\n",
      "    v = make_ripl()\n",
      "    v.execute_program( make_string(bags,colors) )\n",
      "    return v\n",
      "\n",
      "def load_observes(ripl, dataset, make_dataset_args, num_latents, verbose=False):\n",
      "    data1,gtruth_params = make_dataset(dataset, make_dataset_args )\n",
      "    data2 = make_latent_dataset( num_latents )\n",
      "    if verbose:\n",
      "        print '----\\nObserved draws for each bag: '; print_data(bags,data1)\n",
      "        print '\\n(t,color) for latent observes: ', data2\n",
      "\n",
      "    observes = [data_observe(*datum) for datum in data1] + [data_latent_observe(*datum) for datum in data2]\n",
      "    [ripl.observe(*observe) for observe in observes]\n",
      "    return gtruth_params\n",
      "\n",
      "def make_queries(bags,num_latents):\n",
      "    bag_queries = ['(bag_prototype %i)'%bag for bag in range(bags)]\n",
      "    latents = ['(draw_bag %i)'%t for t in range(num_latents)]\n",
      "    return ['hyper_alpha'] + bag_queries + latents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_infer_model(num_latents, verbosity=True):\n",
      "    v = load_model(bags, colors, make_latent_bag_string)\n",
      "    \n",
      "    make_dataset_args = (bags, colors, draws_per_bag, max_alpha_prior)\n",
      "    gtruth_params = load_observes(v, dataset, make_dataset_args, num_latents, verbose=False)\n",
      "    queries = make_queries(bags,num_latents)\n",
      "\n",
      "    if verbosity:\n",
      "        print '\\n\\n INFERENCE \\n --------- \\n Before inference: '\n",
      "    display_compare_queries(v, queries,gtruth_params, verbose = False)\n",
      "    \n",
      "    start = time.time()\n",
      "    v.infer(num_transitions)\n",
      "    elapsed = time.time() - start\n",
      "    \n",
      "    if verbosity:\n",
      "        print 'elapsed: ',elapsed\n",
      "        print '\\n\\nAfter inference: '\n",
      "\n",
      "    result = display_compare_queries(v, queries, gtruth_params, verbose = verbosity)\n",
      "\n",
      "    return v, result\n",
      "\n",
      "\n",
      "def make_series(results,field='logscore'):\n",
      "    pairs = []\n",
      "    for k in sorted(results.keys()):\n",
      "        if isinstance(k,int):\n",
      "            pairs.append( (k, results[k][field] ) )\n",
      "    return pairs\n",
      "   \n",
      "bags, colors = 4,4\n",
      "draws_per_bag = 5\n",
      "max_alpha_prior = 5\n",
      "    \n",
      "num_latents_values = range(1,1000,100)\n",
      "num_transitions = 10\n",
      "dataset = 'even'\n",
      "\n",
      "verbosity = False\n",
      "results = dict(num_transitions = num_transitions)\n",
      "\n",
      "for i,num_latents in enumerate(num_latents_values):\n",
      "    if verbosity or True:\n",
      "        print 'RUN: %i' % i\n",
      "    tic = time.time()\n",
      "    v, result = load_infer_model(num_latents, verbosity=verbosity)\n",
      "    print 'Time ', time.time() - tic\n",
      "    results[num_latents] = result\n",
      "    \n",
      "series_alpha = make_series(results,'mse_alpha')\n",
      "series_ptypes = make_series(results,'mse_ptypes')\n",
      "all_params = dict(bags=bags,colors=colors,draws_per_bag=draws_per_bag,\n",
      "                  max_alpha_prior=max_alpha_prior, num_transitions=num_transitions)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RUN: 0\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.585332155228\n",
        "RUN: 1\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.36111903191\n",
        "RUN: 2\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.83452105522\n",
        "RUN: 3\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.03264498711\n",
        "RUN: 4\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.13346409798\n",
        "RUN: 5\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.31056690216\n",
        "RUN: 6\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.14794206619\n",
        "RUN: 7\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7.03984689713\n",
        "RUN: 8\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.86237215996\n",
        "RUN: 9\n",
        "Time "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.7097568512\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots(1,2,figsize=(10,3))\n",
      "ax[0].scatter(*zip(*series_alpha),label='hyper_alpha mse')\n",
      "ax[1].scatter(*zip(*series_ptypes),c='m',marker='+', label='ptypes mse')\n",
      "[ax[i].set_xlabel('num_latents') for i in range(2) ]\n",
      "[ax[i].set_ylabel('MSE') for i in range(2) ]\n",
      "args=all_params['bags'], all_params['colors'],all_params['draws_per_bag'],all_params['num_transitions']\n",
      "ax[0].set_title('b:%i,c%i,dpb:%i, tr:%i'%args)\n",
      "fig.tight_layout()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAskAAADSCAYAAAC4u12cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0xf92412c>"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "basics of scopes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simplified version of hierarchical bags model\n",
      "\n",
      "model='''\n",
      "[assume hyper_alpha (scope_include  (quote hyper_alpha)  0 \n",
      "                      (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)) ) ]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet hyper_alpha) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 1\n",
      "               (dirichlet hyper_alpha) ) ] \n",
      "\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loop over repetitions of infer_prog, tracking changes \n",
      "def loop_infer(ripl, infer_prog,limit=5):\n",
      "    print 'i:  hyper_alpha,    bag0   bag1'\n",
      "    for i in range(5):\n",
      "        alpha = ripl.sample('hyper_alpha') \n",
      "        bags_0 = [ripl.sample('bag%i'%j)[0] for j in (0,1)]\n",
      "        print '%i:  %.2f, %.2f      %.2f  %.2f'%(i,alpha[0],alpha[1],bags_0[0],bags_0[1])\n",
      "        ripl.infer( infer_prog )\n",
      "\n",
      "# default scope: all vars change        \n",
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(mh default one 10)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "loop_infer() takes at least 2 arguments (1 given)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-88-c6a63bb03235>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_ripl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_program\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mloop_infer\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'(mh default one 10)'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: loop_infer() takes at least 2 arguments (1 given)"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# block only contains bag0\n",
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  5.02, 4.67      0.63  0.59\n",
        "1:  5.02, 4.67      0.62  0.59\n",
        "2:  5.02, 4.67      0.42  0.59\n",
        "3:  5.02, 4.67      0.75  0.59"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  5.02, 4.67      0.62  0.59\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample random member of scope\n",
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(mh prototypes one 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  1.64, 4.87      0.27  0.43\n",
        "1:  1.64, 4.87      0.27  0.52"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2:  1.64, 4.87      0.25  0.13\n",
        "3:  1.64, 4.87      0.20  0.60"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  1.64, 4.87      0.21  0.02\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample all members as scope (blocked proposal)\n",
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(mh prototypes all 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  4.73, 6.01      0.26  0.42\n",
        "1:  4.73, 6.01      0.62  0.64"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2:  4.73, 6.01      0.49  0.35\n",
        "3:  4.73, 6.01      0.50  0.31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  4.73, 6.01      0.51  0.28"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(mh hyper_alpha all 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  0.39, 0.24      0.98  0.08\n",
        "1:  0.39, 0.24      0.98  0.08\n",
        "2:  0.39, 0.24      0.98  0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3:  0.39, 0.24      0.98  0.08\n",
        "4:  0.39, 0.24      0.98  0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(cycle ( (mh prototypes one 5) (mh hyper_alpha one 5) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  3.51, 6.88      0.25  0.30\n",
        "1:  3.62, 6.59      0.25  0.27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2:  3.66, 5.36      0.19  0.29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3:  1.84, 5.46      0.42  0.35"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  2.01, 2.32      0.60  0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = make_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(cycle ( (rejection prototypes one 5) (rejection hyper_alpha one 5) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  3.67, 6.53      0.52  0.28\n",
        "1:  3.43, 6.89      0.56  0.13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2:  4.12, 5.30      0.51  0.13"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3:  2.68, 3.29      0.54  0.19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  6.34, 5.05      0.38  0.50"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(cycle ( (pgibbs prototypes one 10 3) (func_pgibbs hyper_alpha one 10 3) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  6.46, 5.35      0.39  0.39\n",
        "1:  6.98, 4.74      0.37  0.58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2:  1.85, 3.14      0.66  0.16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3:  4.05, 6.10      0.33  0.16"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  5.19, 5.58      0.56  0.27"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# slice kernel needs unbounded support\n",
      "# egibbs needs finite, discrete support \n",
      "# hmc need gradients\n",
      "\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( v, '(cycle ( (pgibbs prototypes one 10 3) (mh hyper_alpha one 1) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i:  hyper_alpha,    bag0   bag1\n",
        "0:  5.76, 6.31      0.49  0.58\n",
        "1:  4.76, 6.04      0.55  0.30"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2:  4.76, 6.04      0.62  0.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3:  3.77, 6.18      0.63  0.25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4:  3.50, 6.98      0.39  0.25"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import *\n",
      "\n",
      "# show dynamic scope. hyper_alpha goes in prototypes scope\n",
      "# NOTE issue of inconsistent blocks\n",
      "\n",
      "model='''\n",
      "[assume hyper_alpha (mem ( lambda () (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)))) ]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''\n",
      "def loop_infer(infer_prog,limit=5):\n",
      "    print 'i: hyper_alpha,  bag0  bag1'\n",
      "    for i in range(5):\n",
      "        alpha = v.sample('(hyper_alpha)') \n",
      "        bags_0 = [v.sample('bag%i'%j)[0] for j in (0,1)]\n",
      "        print '%i: %.2f %.2f ,  %.2f   %.2f'%(i,alpha[0],alpha[1],bags_0[0],bags_0[1])\n",
      "        v.infer( infer_prog )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v=mk_p_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 6.90 0.47 ,  0.98   0.52\n",
        "1: 0.20 0.16 ,  0.99   0.07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2: 1.21 1.00 ,  0.80   0.34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.31 2.19 ,  0.48   0.49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 3.27 2.80 ,  0.52   0.29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#whats going on here?\n",
      "model='''\n",
      "[assume hyper_alpha (mem ( lambda ()\n",
      "                            (scope_include (quote hyper_alpha) 0\n",
      "                              (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)))) )]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 2.59 4.26 ,  0.10   0.33\n",
        "1: 3.08 1.23 ,  0.63   0.63\n",
        "2: 1.99 2.29 ,  0.52   0.38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 2.42 2.24 ,  0.93   0.17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.13 0.36 ,  0.98   0.58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scopes for Latents MOdel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "use mix and pgibbs kernels as in scopes.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model setup\n",
      "bags, colors = 5,3\n",
      "\n",
      "# data setup\n",
      "draws_per_bag = 5\n",
      "dataset = 'conc'\n",
      "num_latents = 16\n",
      "\n",
      "v = load_model(bags, colors, make_latent_bag_string_scopes)\n",
      "load_observes(v, bags, colors, draws_per_bag, num_latents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'load_model' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-a3fd61e0a1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_latents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_latent_bag_string_scopes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mload_observes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraws_per_bag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_latents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "queries = make_queries(bags,num_latents)\n",
      "print 'Before inference: '\n",
      "print_queries(queries)\n",
      "v.infer(5000)\n",
      "print '\\n\\nAfter inference: '\n",
      "print_queries(queries)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}