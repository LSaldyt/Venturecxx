{
 "metadata": {
  "name": "",
  "signature": "sha256:051275dd0a26619323413d582e7c37e3ec85e543f2a2a3e925397906e2b9be4b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plan: once complete, we want to have all utils be importable so that later in tut we don't need to run all cells again. annoying to maintain this. for the friday tutorial, we can wait till all finished. then copy alll utils to a script. (maybe some way to import from a notebook?) best thing would be to have permanent version be in script and just print some of them in the notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduce Bag of Colored Balls Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model with no latents / state estimation\n",
      "\n",
      "def make_simple_bag_string(colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string= '''    \n",
      "    [assume hyper_alpha (array %s)]\n",
      "\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                                (dirichlet hyper_alpha) ) ) ]\n",
      "    ''' % prior\n",
      "    return string\n",
      "\n",
      "\n",
      "def data_observe(bag,color): \n",
      "    return ('(categorical (bag_prototype %i) )'%bag,'atom<%i>'%color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_data(bags,data):\n",
      "    for bag in range(bags):\n",
      "        print '\\nbag: ',bag\n",
      "        print 'colors: ', [el[1] for el in data if el[0]==bag]\n",
      "\n",
      "def make_even_data(bags,colors, draws_per_bag, max_alpha_prior):\n",
      "    data = [(bag,color) for bag in range(bags) for color in range(colors)] * draws_per_bag\n",
      "    params = { '(bag_prototype %i)'%bag:np.array( [1./colors]*colors )  for bag in range(bags)}\n",
      "    params['hyper_alpha'] = [max_alpha_prior]*colors\n",
      "    return data, params\n",
      "\n",
      "def make_conc_data(bags,colors, draws_per_bag, max_alpha_prior):\n",
      "    data = [(bag, np.mod(bag,colors)) for bag in range(bags)] * draws_per_bag\n",
      "    ptypes = [np.zeros(colors) for i in range(bags)]\n",
      "    for bag, zero in enumerate(ptypes):\n",
      "        zero[np.mod(bag,colors)] = 1\n",
      "    params = { '(bag_prototype %i)'%bag:ptype for bag,ptype in zip(range(bags),ptypes)}\n",
      "    params['hyper_alpha'] = [.01]*colors\n",
      "    return data, params\n",
      "\n",
      "def make_dataset(dataset,args):\n",
      "    return make_conc_data(*args) if dataset=='conc' else make_even_data(*args)\n",
      "\n",
      "\n",
      "def display_compare_queries(ripl, queries,gtruth_params, verbose=True):\n",
      "    inf_params = {}\n",
      "    \n",
      "    for q in queries: # print queries\n",
      "        if 'draw_bag' not in q and verbose:\n",
      "            print '%s    :'%q, np.round(ripl.sample(q),2)\n",
      "        inf_params[q] = ripl.sample(q)\n",
      "        \n",
      "    logscore = ripl.get_global_logscore()     \n",
      "    if verbose:\n",
      "        print '\\nLogscore: %.2f' % logscore\n",
      "    \n",
      "    # compare to gtruth\n",
      "    mse_ptypes = 0\n",
      "    mse_ar = lambda xs,ys: np.mean( (np.array(xs) - np.array(ys))**2 )\n",
      "    \n",
      "    for k in inf_params.keys():\n",
      "        \n",
      "        if k=='hyper_alpha':\n",
      "            mse_alpha = mse_ar(inf_params[k], gtruth_params[k])\n",
      "            if verbose:\n",
      "                print 'mse hyper:', mse_alpha\n",
      "        elif 'proto' in k:\n",
      "            mse_ptypes += mse_ar(inf_params[k], gtruth_params[k])\n",
      "    if verbose:\n",
      "        print 'mse ptypes:', mse_ptypes\n",
      "    \n",
      "    return dict(logscore=logscore, mse_alpha=mse_alpha, mse_ptypes=mse_ptypes)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model setup\n",
      "max_alpha_prior = 5\n",
      "bags, colors = 5,3\n",
      "v=make_ripl()\n",
      "simple_bag_model = make_simple_bag_string(colors)\n",
      "print 'Model: ',simple_bag_model,'\\n'\n",
      "v.execute_program(simple_bag_model)\n",
      "\n",
      "# data setup\n",
      "draws_per_bag = 5\n",
      "dataset = 'conc'\n",
      "data, gtruth_params = make_dataset(dataset,(bags, colors, draws_per_bag, max_alpha_prior))\n",
      "\n",
      "print '----\\nObserved draws for each bag: ' \n",
      "print_data(bags,data)\n",
      "\n",
      "observes = [data_observe(*datum) for datum in data]\n",
      "out = [v.observe(*observe) for observe in observes]\n",
      "queries = ['hyper_alpha'] + ['(bag_prototype %i)'%bag for bag in range(bags)]\n",
      "\n",
      "print '\\n\\n INFERENCE \\n --------- \\n Before inference: '\n",
      "display_compare_queries(v,queries,gtruth_params)\n",
      "v.infer(2000)\n",
      "print '\\n\\nAfter inference: '\n",
      "display_compare_queries(v,queries,gtruth_params)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Model:      \n",
        "    [assume hyper_alpha (array (uniform_continuous 0.01 5) (uniform_continuous 0.01 5) (uniform_continuous 0.01 5))]\n",
        "\n",
        "    [assume bag_prototype (mem (lambda (bag)\n",
        "                                (dirichlet hyper_alpha) ) ) ]\n",
        "     \n",
        "\n",
        "----\n",
        "Observed draws for each bag: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "bag:  0\n",
        "colors:  [0, 0, 0, 0, 0]\n",
        "\n",
        "bag:  1\n",
        "colors:  [1, 1, 1, 1, 1]\n",
        "\n",
        "bag:  2\n",
        "colors:  [2, 2, 2, 2, 2]\n",
        "\n",
        "bag:  3\n",
        "colors:  [0, 0, 0, 0, 0]\n",
        "\n",
        "bag:  4\n",
        "colors:  [1, 1, 1, 1, 1]\n",
        "\n",
        "\n",
        " INFERENCE \n",
        " --------- \n",
        " Before inference: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hyper_alpha    : [ 2.29  3.07  1.77]\n",
        "(bag_prototype 0)    : [ 0.14  0.32  0.54]\n",
        "(bag_prototype 1)    : [ 0.31  0.53  0.16]\n",
        "(bag_prototype 2)    : [ 0.57  0.41  0.03]\n",
        "(bag_prototype 3)    : [ 0.38  0.54  0.07]\n",
        "(bag_prototype 4)    : [ 0.68  0.23  0.09]\n",
        "\n",
        "Logscore: -20.72"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "mse hyper: 5.89771381917\n",
        "mse ptypes: 1.55133506158\n",
        "\n",
        "\n",
        "After inference: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "hyper_alpha    : [ 0.32  0.33  0.08]\n",
        "(bag_prototype 0)    : [ 0.85  0.    0.15]\n",
        "(bag_prototype 1)    : [ 0.21  0.79  0.  ]\n",
        "(bag_prototype 2)    : [ 0.    0.12  0.88]\n",
        "(bag_prototype 3)    : [ 0.87  0.13  0.  ]\n",
        "(bag_prototype 4)    : [ 0.01  0.99  0.  ]\n",
        "\n",
        "Logscore: 23.62\n",
        "mse hyper: 0.0686072739807\n",
        "mse ptypes: 0.0656901446927\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "{'logscore': 23.61723508260917,\n",
        " 'mse_alpha': 0.068607273980699005,\n",
        " 'mse_ptypes': 0.06569014469270204}"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Latents, inference without scopes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Model with latents, still no scopes. show that inference doesn't do so well as we add latents"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import MRipl, make_lite_church_prime_ripl, venture\n",
      "from venture.venturemagics.ip_parallel import make_puma_church_prime_ripl as make_ripl\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_latent_bag_string(bags,colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string=''' \n",
      "    [assume atom_number (lambda (atom) (+ 0 atom) ) ]\n",
      "    \n",
      "    [assume bags %i]\n",
      "    [assume hyper_alpha (array %s)]\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                                 (dirichlet hyper_alpha) ) ) ]\n",
      "\n",
      "    [assume draw_bag (mem (lambda (t)\n",
      "                            (atom_number\n",
      "                                (uniform_discrete 0 bags) ) ) )]\n",
      "\n",
      "    [assume t_color (mem (lambda (t) \n",
      "                           (categorical \n",
      "                             (bag_prototype (draw_bag t)) ) ) )]\n",
      "\n",
      "    ''' % (bags,prior)\n",
      "    return string\n",
      "\n",
      "def make_latent_bag_string_scopes(bags,colors):\n",
      "    prior = ' '.join( ['(uniform_continuous 0.01 5)']*colors )\n",
      "    string='''\n",
      "    [assume atom_number (lambda (atom) (+ 0 atom) ) ]\n",
      "\n",
      "    [assume bags %i]\n",
      "\n",
      "    [assume hyper_alpha (scope_include (quote hyper_alpha) 0\n",
      "                            (array %s) )]\n",
      "\n",
      "    [assume bag_prototype (mem (lambda (bag)\n",
      "                               (scope_include (quote prototypes) bag\n",
      "                                   (dirichlet hyper_alpha) ) ) )]\n",
      "\n",
      "    [assume draw_bag (mem (lambda (t)\n",
      "                           (scope_include (quote latents) t\n",
      "                             (atom_number\n",
      "                               (uniform_discrete 0 bags) ) )) )]\n",
      "\n",
      "    [assume t_color (mem (lambda (t) \n",
      "                           (categorical \n",
      "                             (bag_prototype (draw_bag t) ) ) )) ]\n",
      "\n",
      "    '''%(bags,prior)\n",
      "    return string\n",
      "\n",
      "\n",
      "def data_observe(bag,color): \n",
      "    return ('(categorical (bag_prototype %i) )'%bag,'atom<%i>'%color)\n",
      "\n",
      "def data_latent_observe(t,color):\n",
      "    return ('(t_color %i)'%t, 'atom<%i>'%color)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_latent_dataset(num_latents):\n",
      "    data = []\n",
      "    for t in range(num_latents):\n",
      "        color = 0 if (t <=.5*num_latents) else 1\n",
      "        data.append( (t,color) )\n",
      "    return data\n",
      "\n",
      "def load_model(bags,colors,make_string):\n",
      "    v = make_ripl()\n",
      "    v.execute_program( make_string(bags,colors) )\n",
      "    return v\n",
      "\n",
      "def load_observes(ripl, dataset, make_dataset_args, num_latents, verbose=False):\n",
      "    data1,gtruth_params = make_dataset(dataset, make_dataset_args )\n",
      "    data2 = make_latent_dataset( num_latents )\n",
      "    if verbose:\n",
      "        print '----\\nObserved draws for each bag: '; print_data(bags,data1)\n",
      "        print '\\n(t,color) for latent observes: ', data2\n",
      "\n",
      "    observes = [data_observe(*datum) for datum in data1] + [data_latent_observe(*datum) for datum in data2]\n",
      "    [ripl.observe(*observe) for observe in observes]\n",
      "    return gtruth_params\n",
      "\n",
      "def make_queries(bags,num_latents):\n",
      "    bag_queries = ['(bag_prototype %i)'%bag for bag in range(bags)]\n",
      "    latents = ['(draw_bag %i)'%t for t in range(num_latents)]\n",
      "    return ['hyper_alpha'] + bag_queries + latents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_infer_model(num_latents, verbosity=True):\n",
      "    \n",
      "    bags, colors = 4,4\n",
      "    draws_per_bag = 5\n",
      "    max_alpha_prior = 5\n",
      "\n",
      "    v = load_model(bags, colors, make_latent_bag_string)\n",
      "    \n",
      "    make_dataset_args = (bags, colors, draws_per_bag, max_alpha_prior)\n",
      "    gtruth_params = load_observes(v, dataset, make_dataset_args, num_latents, verbose=False)\n",
      "    queries = make_queries(bags,num_latents)\n",
      "\n",
      "    if verbosity:\n",
      "        print '\\n\\n INFERENCE \\n --------- \\n Before inference: '\n",
      "    display_compare_queries(v, queries,gtruth_params, verbose = False)\n",
      "    \n",
      "    start = time.time()\n",
      "    v.infer(num_transitions)\n",
      "    elapsed = time.time() - start\n",
      "    \n",
      "    if verbosity:\n",
      "        print 'elapsed: ',elapsed\n",
      "        print '\\n\\nAfter inference: '\n",
      "\n",
      "    result = display_compare_queries(v, queries, gtruth_params, verbose = verbosity)\n",
      "\n",
      "    return v, result\n",
      "\n",
      "\n",
      "def make_series(results,field='logscore'):\n",
      "    pairs = []\n",
      "    for k in sorted(results.keys()):\n",
      "        if isinstance(k,int):\n",
      "            pairs.append( (k, results[k][field] ) )\n",
      "    return pairs\n",
      "   \n",
      "    \n",
      "num_latents_values = range(1,3000,900)\n",
      "num_transitions = 500\n",
      "dataset = 'even'\n",
      "\n",
      "verbosity = False\n",
      "results = dict(num_transitions = num_transitions)\n",
      "\n",
      "for i,num_latents in enumerate(num_latents_values):\n",
      "    if verbosity or True:\n",
      "        print 'RUN: %i' % i\n",
      "    tic = time.time()\n",
      "    v, result = load_infer_model(num_latents, verbosity=verbosity)\n",
      "    print 'Time ', time.time() - tic\n",
      "    results[num_latents] = result\n",
      "    \n",
      "series_alpha = make_series(results,'mse_alpha')\n",
      "series_ptypes = make_series(results,'mse_ptypes')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "RUN: 0\n",
        "Time "
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig,ax = plt.subplots(1,2,figsize=(10,3))\n",
      "ax[0].scatter(*zip(*series_alpha),label='hyper_alpha mse')\n",
      "ax[1].scatter(*zip(*series_ptypes),c='m',marker='+', label='ptypes mse')\n",
      "[ax[i].set_xlabel('num_latents') for i in range(2) ]\n",
      "[ax[i].set_ylabel('MSE') for i in range(2) ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "[<matplotlib.text.Text at 0x1190044c>, <matplotlib.text.Text at 0x1191876c>]"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAADVCAYAAAD5GOxFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc0MiAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0xd48af0c>"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "basics of scopes"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# simplified version of hierarchical bags model\n",
      "\n",
      "from venture.venturemagics.ip_parallel import *\n",
      "model='''\n",
      "[assume hyper_alpha (scope_include  (quote hyper_alpha)  0 \n",
      "                      (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)) ) ]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet hyper_alpha) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 1\n",
      "               (dirichlet hyper_alpha) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# loop over repetitions of infer_prog, tracking changes \n",
      "def loop_infer(infer_prog,limit=5):\n",
      "    print 'i: hyper_alpha,  bag0  bag1'\n",
      "    for i in range(5):\n",
      "        alpha = v.sample('hyper_alpha') \n",
      "        bags_0 = [v.sample('bag%i'%j)[0] for j in (0,1)]\n",
      "        print '%i: %.2f %.2f ,  %.2f   %.2f'%(i,alpha[0],alpha[1],bags_0[0],bags_0[1])\n",
      "        v.infer( infer_prog )\n",
      "\n",
      "# default scope: all vars change        \n",
      "v = mk_p_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh default one 10)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 2.85 5.89 ,  0.57   0.22\n",
        "1: 2.52 4.82 ,  0.61   0.25\n",
        "2: 2.88 2.74 ,  0.61   0.21"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 0.91 2.03 ,  0.61   0.27\n",
        "4: 2.18 2.03 ,  0.80   0.24"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# block only contains bag0\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 1.96 5.60 ,  0.16   0.39\n",
        "1: 1.96 5.60 ,  0.39   0.39\n",
        "2: 1.96 5.60 ,  0.61   0.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.96 5.60 ,  0.47   0.39\n",
        "4: 1.96 5.60 ,  0.42   0.39"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample random member of scope\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes one 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 3.76 4.43 ,  0.43   0.28\n",
        "1: 3.76 4.43 ,  0.78   0.14\n",
        "2: 3.76 4.43 ,  0.78   0.61\n",
        "3: 3.76 4.43 ,  0.78   0.31"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 3.76 4.43 ,  0.78   0.18\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sample all members as scope (blocked proposal)\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes all 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 4.08 2.76 ,  0.77   0.69\n",
        "1: 4.08 2.76 ,  0.45   0.47\n",
        "2: 4.08 2.76 ,  0.44   0.56"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 4.08 2.76 ,  0.73   0.61\n",
        "4: 4.08 2.76 ,  0.82   0.69"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh hyper_alpha all 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 5.78 2.86 ,  0.58   0.88\n",
        "1: 5.84 1.58 ,  0.58   0.88\n",
        "2: 6.47 2.15 ,  0.58   0.88\n",
        "3: 6.20 3.67 ,  0.58   0.88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 6.20 3.67 ,  0.58   0.88\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(cycle ( (mh prototypes one 5) (mh hyper_alpha one 5) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 1.84 4.07 ,  0.12   0.32\n",
        "1: 2.31 6.32 ,  0.37   0.10\n",
        "2: 2.45 3.06 ,  0.56   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.27 3.51 ,  0.36   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.27 3.51 ,  0.36   0.19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vl=mk_l_ripl()\n",
      "vl.execute_program(model)\n",
      "loop_infer( '(cycle ( (rejection prototypes one 5) (rejection hyper_alpha one 5) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 1.27 3.51 ,  0.46   0.18\n",
        "1: 1.64 6.78 ,  0.20   0.28\n",
        "2: 1.48 6.91 ,  0.20   0.23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.48 6.91 ,  0.20   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.48 6.91 ,  0.34   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(cycle ( (func_pgibbs prototypes one 10 3) (func_pgibbs hyper_alpha one 10 3) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 5.24 0.32 ,  0.99   0.60\n",
        "1: 2.96 1.07 ,  0.99   0.79"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2: 1.81 0.16 ,  0.99   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 2.31 0.61 ,  1.00   0.08"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 2.31 0.61 ,  1.00   0.23"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# slice kernel needs unbounded support\n",
      "# egibbs needs finite, discrete support \n",
      "# hmc need gradients\n",
      "\n",
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(cycle ( (func_pgibbs prototypes one 5) (mh hyper_alpha one 1) ) 1)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from venture.venturemagics.ip_parallel import *\n",
      "\n",
      "# show dynamic scope. hyper_alpha goes in prototypes scope\n",
      "# NOTE issue of inconsistent blocks\n",
      "\n",
      "model='''\n",
      "[assume hyper_alpha (mem ( lambda () (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)))) ]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''\n",
      "def loop_infer(infer_prog,limit=5):\n",
      "    print 'i: hyper_alpha,  bag0  bag1'\n",
      "    for i in range(5):\n",
      "        alpha = v.sample('(hyper_alpha)') \n",
      "        bags_0 = [v.sample('bag%i'%j)[0] for j in (0,1)]\n",
      "        print '%i: %.2f %.2f ,  %.2f   %.2f'%(i,alpha[0],alpha[1],bags_0[0],bags_0[1])\n",
      "        v.infer( infer_prog )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v=mk_p_ripl()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 6.90 0.47 ,  0.98   0.52\n",
        "1: 0.20 0.16 ,  0.99   0.07"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2: 1.21 1.00 ,  0.80   0.34"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 1.31 2.19 ,  0.48   0.49"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 3.27 2.80 ,  0.52   0.29"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#whats going on here?\n",
      "model='''\n",
      "[assume hyper_alpha (mem ( lambda ()\n",
      "                            (scope_include (quote hyper_alpha) 0\n",
      "                              (array (uniform_continuous 0.01 7) (uniform_continuous 0.01 7)))) )]\n",
      "\n",
      "[assume bag0 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ]\n",
      "               \n",
      "[assume bag1 (scope_include (quote prototypes) 0\n",
      "               (dirichlet (hyper_alpha) ) ) ] \n",
      "               \n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag0) atom<0>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "[observe (categorical bag1) atom<1>]\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v.clear()\n",
      "v.execute_program(model)\n",
      "loop_infer( '(mh prototypes 0 5)' )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i: hyper_alpha,  bag0  bag1\n",
        "0: 2.59 4.26 ,  0.10   0.33\n",
        "1: 3.08 1.23 ,  0.63   0.63\n",
        "2: 1.99 2.29 ,  0.52   0.38"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3: 2.42 2.24 ,  0.93   0.17"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4: 1.13 0.36 ,  0.98   0.58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scopes for Latents MOdel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "use mix and pgibbs kernels as in scopes.py."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# model setup\n",
      "bags, colors = 5,3\n",
      "\n",
      "# data setup\n",
      "draws_per_bag = 5\n",
      "dataset = 'conc'\n",
      "num_latents = 16\n",
      "\n",
      "v = load_model(bags, colors, make_latent_bag_string_scopes)\n",
      "load_observes(v, bags, colors, draws_per_bag, num_latents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'load_model' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-5-a3fd61e0a1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_latents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmake_latent_bag_string_scopes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mload_observes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraws_per_bag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_latents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'load_model' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "queries = make_queries(bags,num_latents)\n",
      "print 'Before inference: '\n",
      "print_queries(queries)\n",
      "v.infer(5000)\n",
      "print '\\n\\nAfter inference: '\n",
      "print_queries(queries)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}