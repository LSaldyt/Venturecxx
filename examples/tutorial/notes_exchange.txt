;; This buffer is for notes you don't want to save, and for Lisp evaluation.
;; If you want to create a file, visit that file with C-x C-f,
;; then enter the text in that file's own buffer.

notes towards tutorial on exchangeability, collapsed samplers, AAA

1. Venture thunks output exchangeable sequence of RVs
  - re-running whole Venture program will give iid sequence
  - most venture builtin SPs will give iid sequence
  - easy to write 'mixture model' thunk where values are not iid if you don't know the latent parameter values
  - make_dir_mult constructs a thunk that has non-iid but exchangeable sequence of RVs as output. compare linear scaling of writing dir_mult by hand vs. using primitive. due to AAA. (but could also use sufficient statistics for same linear advantage). no ability in venture to read off the state of the dir_mult directly. at best could use SAMPLE and take an average. can read off counts outside venture by looking at type of the procedure.

2. Exchangeability: some general results
 - computable mixture models correspond to computable exchangeable sequence of RVs. any relevant complexity results?
 - polya urn process: simple scheme that defines a sequence of RVs that are exchangeable (distribution on n-th RV only depends on the composition of urn at that point, which depends only on the previous draws from the urn and not their order). nb: for urn with (1 black,1white) starting point, each RV is unconditionally the same as the first one, namely, (flip .5). obvious here by symmetry, but this holds generally (iirc) for all finite urn processes. variables are identical but not independent. exchangeability is about dependency of variables.
 - urn processes are exchangeable and so correspond to mixture model. rough idea of de finetti for urn: path dependence of urn, as n gets larger, it behaves increasingly like a fixed coin/dice. need an increasingly long and unlikely run to change bias. assuming that sequence of RVs does converge, we can ask what probability of different coin weights are as function of initial distribution of balls in urn. given by beta, dirichlet, dirichlet process.

3. Computational upshot: 
- use simulated urn process in place of mixture model. simple to code in impure language by mutating the counts. (exchangeability means the mutation is reversible and can be reversed in any order). could write this without mutation by computing the counts from the trace (not possible in venture currently). 

- explanation of how the update is done in constant time in trace (exploiting exchangeability)

- Q: in cases where uncollapsed sampler is available and can be done in constant time using sufficient statistics, what are the benefits of using the collapsed sampler? (i'm not sure here of what trade-offs are).

- in non-parameteric case, what are trade-offs of using stick-breaking construction vs. crp construction? if you defined a stick-breaking construction in venture, this would scale poorly. (without sufficient statistics, you'll have linear scaling at least to compute likelihood of a given stick-length/coin-weight.) but what about efficiency of a new builtin that organizes computation to try to avoid linear scaling in observations? 


references:
probmods simple exchangeability examples. examples of pick-a-stick for concentrating continuous distribution

freer and roy on computable random sequences. 

vkm thesis on generalized purity and exchangeable sequences.

bernardo and smith - bayesian theory.
extensive but very dry discussion of exchangeability. part of discussion is to use de finetti to support bayesian practice. supposed problem is that bayesians introduce hidden, unobservable parameters and this is not kosher from extreme positivist standpoint (where only observations exist). a less general point would be: you should only introduce a parameter when you have good reason to think it corresponds to something real. if you see a series a balls coming from an urn, you can't assume there's a fixed parameter 'ratio of white:black balls'. maybe the urn is a polya urn whose state changes every draw. (interesting to think about this in context of fundamental physics, where we have no direct access to objects underlying observations). there's a practical upshot: there is a simple intervention that changes the state of a normal urn (with iid draws with replacement), while there's no comparable simple intervention on the polya urn. 

de finetti is supposed to help because as long as you believe data is exchangeable (infite exchangeable -- finite case is more complex) you can use a mixture model as a purely mathematical convenience, with the parameter not corresponding to a fixed part of the world but just to a mathematical limit of a part of the world (as times goes to infinity). this is similar to vN-Morgenstern showing that if you satisfy decision-theory axioms then we can (as mathematical convenience) model your decisions as maximizing a real-value utility function. 

pearl and others have investigate relation of exchangeability and causation:
Pearl, Causality 2nd Edition
Zhang, Exchangeability and Invariance: A Causal Theory
Diaconis, Failure of Infinite Exchangeability and results for finite versions. 

 

notes on venture and stats :

[assume x (uniform_discrete -100 100)]
[assume y (normal x .01)]

1. if you get [observe (normal x epsilon) 50.], then do [infer (mh scope_xy all transitions)] you will infer that y is close to 50. this is true as the noise *epsilon* on *x* tends to zero. But if you do an observe with no noise, [observe x 50], then you get an exception. (Currently not a helpful exception). 

suppose you do observe the value of *x* directly. in venture you'd have to model this by re-writing the model. this is reasonable but goes against the use of venture for online inference on heterogenous expressions. (suppose you want to update your model in response to a series of observations of unknown expressions in your model. then there are discontinuities in the kinds of expressions you can observe.)


2. improper priors: 
Not clear if you can sample from them. However, even if you can't, you can get initial sample from the best available proxy and then do MH using the improper prior. Afaict this is what STAN does. Could Venture not do the same? One problem is that Venture doesn't specify the form that *observes* can take. So given an improper prior, it would be possible to add an *observe* that leads to an improper posterior. Generally, we see a conflict between having flexible *observes* that aren't of a pre-specified form and having improper priors. [If we condition 

3. Jeffrey's Priors
Venture supports some Jeffreys priors (e.g. Beta(.5,.5)), but not the improper ones. Generally, Jeffreys priors depend on computing the Fisher information matrix for the parameters of the model. The Fisher information depends on taking an expectation w.r.t possible datasets generated by the model. This leads to different values for the Fisher information depending on which experiment is done. For example, whether we decide to flip a coin 10 times or to flip a coin till it comes up tails.

 So Jeffreys priors also depend on which experiment is done. Venture programs don't specify what form 'observes' will take and so it's not clear if Jeffrey

Related point is that estimation theory depends on there being iid samples. If we don't have a set form for observes, then specific results on asymptotics of variance of an estimator will not be useful.
















notes rao-blackwell:
info theory: if estimator is a func of SS then it's the same. if not, estimator must vary on datasets with same SS. if so, it's using some spurious aspect of data (irrelevant to param theta). like a weather forecaster incorporating horoscope/psychic into a forecast that also depends on some (but not all) recent data in timeseries. 

one idea: you'd like to reverse taking into account the spurious data. if you had lost the info about which world you're in (and only kep the suff stat) and you had to use the estimator, you'd have to average over it. so RB is what you'd get if you were forced to do this. (after taking the av, you remove any info about the estimate about the spurious data source. estimate now is only a function of SS. 

[info theory and estimation. the poisson estimator that only takes sum of a subset of the data  --- not clear how easy to combine with info theory. intuitively, the estimator that only uses subset of data is bad because it fails to use variables that are informative about the parameter. every X_i which is drawn iid from the poisson lambda gives certain amount of into about lam. can't formalize coz lam ain't a random variable. we can talk about info gain of observing X, average DROP in entropy from prior to posterior from observing X_i. But if you already know lam then there's no drop. So we need some prior assumption. 

clear idea: let estimator be unbiased and expectation of estimator (= value of parameter) = 0. we then show that variance goes down when we RB the estimator. take the old estimator's variance, which is just weighted sum of g(x)^2 where x is data vector. under RB, we take all x's with same SS (i.e. in same cell of partition) and make their estimate E g(x) / S=s, and then square this estimate. so now the squaring takes place outside the sum. this make the sum smaller by convexity. (suggests that a version of RB will be more general and won't depend on variance but just on the squaring making for convexity). 
question of whether we could instead condition on some other function of the data and still get same result. 










