Venture is a higher-order probabilistic programming platform that aims
to be sufficiently expressive, extensible, and efficient for
general-purpose use.  We intend Venture to be useful in all domains
and areas where software must deal with uncertainty or potentially
incomplete models.  This includes the traditional fields of machine
learning, statistics, and artificial intelligence; traditional domains
where those methods have found fruit, such as robotics, finance,
economics, computer vision, scientific simulation, political science,
and natural language processing; and potential applications in new
domains where probabilistic modeling has not yet been applied
successfully.

To this end, we are designing Venture to

- be able to express any probabilistic process as a model and

- be able to carry out any inference strategy on any model to which it
  applies,

- including nesting modeling inside inference and inference inside
  modeling,

- supporting a separation of concerns that permits implementing
  generic inference strategies as reusable library procedures,

- while retaining all the efficiency of custom implementations,
  including opportunities for parallelism, and

- interfacing smoothly with foreign or legacy model or inference
  programs.

In Venture, inference is programmable. Venture programmers can
implement custom inference strategies without modifying the code that
represents modeling assumptions, observed data, and queries. Venture
provides a standard inference library that includes a broad class of
exact and approximate techniques as well as inference combinators that
make it possible to assemble complex inference strategies out of
reusable components. Some inference strategies, such as single-site
Metropolis Hastings and likelihood weighting, require only a single
inference instruction, yet apply to all programs and have strong
convergence guarantees. These can be thought of as "automatic". Other
strategies, such as blocked Gibbs sampling, will reflect the details
of individual problem instances. Many of these strategies are far
easier to customize (given a standard inference library) than write
from scratch.

This approach leads to different expressiveness, scalability, and
constant-factor performance desiderata than approaches based on
machine-assisted choice among a set of black-box solvers. For example,
greater modeling expressiveness can improve tractability in this
setting, by making it possible for programmers to express alternative
"inference-optimized" versions of their models that are more amenable
to efficient inference. Constant-factor performance can be achieved by
improving the runtime system and writing a compiler, or by making it
easy for users to incrementally migrate performance-sensitive regions
of their model, query, dataset and/or inference strategy into
optimized foreign code.

In pursuit of our ambitious set of goals we have, to date, deferred
improving the constant factors of performance (both execution time and
memory usage) of the Venture platform.

http://probcomp.csail.mit.edu/venture/
